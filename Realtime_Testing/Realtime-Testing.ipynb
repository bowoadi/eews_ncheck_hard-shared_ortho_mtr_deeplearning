{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1596444015873,
     "user": {
      "displayName": "Satriawan Rasyid Purnama",
      "photoUrl": "",
      "userId": "04512086703035254103"
     },
     "user_tz": -420
    },
    "id": "umr8IcF2hgci"
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>No</th>\n",
       "      <th>filename</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hr</th>\n",
       "      <th>Min</th>\n",
       "      <th>Sec</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Mag</th>\n",
       "      <th>...</th>\n",
       "      <th>seconds</th>\n",
       "      <th>time</th>\n",
       "      <th>foldername</th>\n",
       "      <th>PA</th>\n",
       "      <th>cluster_class</th>\n",
       "      <th>norm_lat</th>\n",
       "      <th>norm_long</th>\n",
       "      <th>norm_magnitude</th>\n",
       "      <th>norm_depth</th>\n",
       "      <th>Depth(km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>670</td>\n",
       "      <td>20170428_014929</td>\n",
       "      <td>28/04/2017</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>-8.08697</td>\n",
       "      <td>111.716</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>16.024</td>\n",
       "      <td>16.024</td>\n",
       "      <td>20170428_014929crop</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.08697</td>\n",
       "      <td>111.716</td>\n",
       "      <td>3.4</td>\n",
       "      <td>126.932</td>\n",
       "      <td>126.932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   No         filename        Date  Hr  Min  Sec      Lat  \\\n",
       "0         636  670  20170428_014929  28/04/2017   1   49   29 -8.08697   \n",
       "\n",
       "      Long  Mag  ...  seconds    time           foldername   PA  \\\n",
       "0  111.716  3.4  ...   16.024  16.024  20170428_014929crop  200   \n",
       "\n",
       "   cluster_class  norm_lat  norm_long  norm_magnitude norm_depth  Depth(km)  \n",
       "0              1  -8.08697    111.716             3.4    126.932    126.932  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMAGES PATH\n",
    "dataset_path = '../data/ENZ_crop/'\n",
    "# CSV PATH\n",
    "df = pd.read_csv('../data/data_618_final.csv')\n",
    "\n",
    "# CHANGE COLUMN NAME\n",
    "df['Depth(km)'] = df['Depth']\n",
    "df.drop('Depth',axis=1,inplace=True)\n",
    "df['foldername'] = df['filename'].apply(lambda x: x+'crop')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>No</th>\n",
       "      <th>filename</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hr</th>\n",
       "      <th>Min</th>\n",
       "      <th>Sec</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Mag</th>\n",
       "      <th>...</th>\n",
       "      <th>seconds</th>\n",
       "      <th>time</th>\n",
       "      <th>foldername</th>\n",
       "      <th>PA</th>\n",
       "      <th>cluster_class</th>\n",
       "      <th>norm_lat</th>\n",
       "      <th>norm_long</th>\n",
       "      <th>norm_magnitude</th>\n",
       "      <th>norm_depth</th>\n",
       "      <th>Depth(km)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>636</td>\n",
       "      <td>670</td>\n",
       "      <td>20170428_014929</td>\n",
       "      <td>28/04/2017</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>-8.08697</td>\n",
       "      <td>111.716</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>16.024</td>\n",
       "      <td>16.024</td>\n",
       "      <td>20170428_014929crop</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.08697</td>\n",
       "      <td>111.716</td>\n",
       "      <td>3.4</td>\n",
       "      <td>126.932</td>\n",
       "      <td>126.932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   No         filename        Date  Hr  Min  Sec      Lat  \\\n",
       "0         636  670  20170428_014929  28/04/2017   1   49   29 -8.08697   \n",
       "\n",
       "      Long  Mag  ...  seconds    time           foldername   PA  \\\n",
       "0  111.716  3.4  ...   16.024  16.024  20170428_014929crop  200   \n",
       "\n",
       "   cluster_class  norm_lat  norm_long  norm_magnitude norm_depth  Depth(km)  \n",
       "0              1  -8.08697    111.716             3.4    126.932    126.932  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## SELECT PA VALUES\n",
    "df['PA'] = df.apply(lambda x: x['flag_start']-75 if x['Unpick']!=0 else x['PA_start']-75,axis=1)\n",
    "df = df[df.PA>=0]\n",
    "df['PA'] = df.PA.apply(lambda x: int(x))\n",
    "\n",
    "## CREATE PA DICTIONARIES\n",
    "PA = {}\n",
    "for i in range(df.shape[0]):\n",
    "    PA[df.filename.values[i]+\"crop\"] = df.PA.values[i]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Lat: -8.086969999999999 \t\tMin Lat: -8.086969999999999\n",
      "Max Long: 111.716 \tMin Long: 111.716\n",
      "Max Depth(km): 126.932 \tMin Depth(km): 126.932\n",
      "Max Mag: 3.4 \t\tMin Mag: 3.4\n",
      "Max Time: 16.024 \t\tMin Time: 16.024\n"
     ]
    }
   ],
   "source": [
    "print('Max Lat:',df['Lat'].max(),'\\t\\tMin Lat:',df['Lat'].min())\n",
    "print('Max Long:',df['Long'].max(),'\\tMin Long:',df['Long'].min())\n",
    "print('Max Depth(km):',df['Depth(km)'].max(),'\\tMin Depth(km):',df['Depth(km)'].min())\n",
    "print('Max Mag:',df['Mag'].max(),'\\t\\tMin Mag:',df['Mag'].min())\n",
    "print('Max Time:',df['time'].max(),'\\t\\tMin Time:',df['time'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NORMALIZATION\n",
    "#coef_norm_lat = 3 \n",
    "coef_norm_lat = df['Lat'].max() - df['Lat'].min() # min-max-normalization 0-1 \n",
    "\n",
    "#coef_norm_long = 3\n",
    "coef_norm_long = df['Long'].max() - df['Long'].min() # min-max-normalization 0-1\n",
    "\n",
    "#coef_norm_mag = 6\n",
    "coef_norm_magnitude = df['Mag'].max() - df['Mag'].min() # min-max-normalization 0-1\n",
    "\n",
    "#coef_norm_mag = 6\n",
    "coef_norm_depth = df['Depth(km)'].max() - df['Depth(km)'].min() # min-max-normalization 0-1\n",
    "\n",
    "#coef_norm_time = 6\n",
    "coef_norm_time = df['time'].max() - df['time'].min() # min-max-normalization 0-1\n",
    "\n",
    "df['norm_lat'] = df['Lat'].apply(lambda x: (x - df['Lat'].min()) / coef_norm_lat)\n",
    "df['norm_long'] = df['Long'].apply(lambda x: (x - df['Long'].min()) / coef_norm_long)\n",
    "df['norm_magnitude'] = df['Mag'].apply(lambda x: (x - df['Mag'].min()) / coef_norm_magnitude)\n",
    "df['norm_depth'] = df['Depth(km)'].apply(lambda x: (x - df['Depth(km)'].min()) / coef_norm_depth)\n",
    "df['norm_time'] = df['time'].apply(lambda x: (x - df['time'].min()) / coef_norm_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2515,
     "status": "ok",
     "timestamp": 1596444300875,
     "user": {
      "displayName": "Satriawan Rasyid Purnama",
      "photoUrl": "",
      "userId": "04512086703035254103"
     },
     "user_tz": -420
    },
    "id": "p16CAaKOjAFh",
    "outputId": "5e212699-10b4-40d8-a7ef-3a5a12aab601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 423, 3)\n",
      "(256, 192, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dfdaae1f88>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#get image size from dataset\n",
    "img = load_img(dataset_path+'20170428_014929crop/'+'000000_20170428_014929_crop.png')  # this is a PIL image\n",
    "x = img_to_array(img)\n",
    "print(x.shape)\n",
    "plt.figure(figsize=(5,5))\n",
    "x = cv2.resize(x, (192, 256))\n",
    "# x = cv2.resize(x, (x.shape[1]//2,x.shape[0]//2))\n",
    "print(x.shape)\n",
    "plt.imshow(x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pembagian Dataset:\n",
      "TRAIN: 0\n",
      "VALIDATION: 1\n",
      "TEST: 1\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../data/data_test_val_final2.csv')\n",
    "test_df = df[df.foldername.isin(test_df.foldername.values)]  ## TESTSET is only 1 example data avaliable\n",
    "train_df = df[~df.foldername.isin(test_df.foldername.values)] ## TRAINSET might 0 because the data is unsharable\n",
    "\n",
    "list_folder_for_train = [__ for __ in train_df.foldername.values]\n",
    "list_folder_for_validation = [__ for __ in test_df.foldername.values]\n",
    "list_folder_for_test = list_folder_for_validation\n",
    "\n",
    "print()\n",
    "print('Pembagian Dataset:')\n",
    "print('TRAIN:',len(list_folder_for_train))\n",
    "print('VALIDATION:',len(list_folder_for_validation))\n",
    "print('TEST:',len(list_folder_for_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check redundant in train\n",
    "for __ in list_folder_for_train:\n",
    "    if __ in  list_folder_for_test+list_folder_for_validation:\n",
    "        print(__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xeV9TB7YkCHb"
   },
   "outputs": [],
   "source": [
    "#make folder test \n",
    "model_save_path = \"./result_test/EXPERIMENT_001/\"\n",
    "if not os.path.exists(model_save_path):\n",
    "    os.mkdir(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustering : ./model_test/clustering/cluster32_model.sav\n",
      "multi_lat_long_depth_magnitude_time : ./model_test/multi_lat_long_depth_magnitude_time/EFFICIENTNETB5_multioutput_lat_long_depth_magnitude_time_5s-6s_mae_B_4_1x1024orto_paral512-256_best.h5\n",
      "parrival : ./model_test/parrival/EFFICIENTNETB5_parival_5-6_ENZ_final_all_ep50_batch32_#1_best.h5\n"
     ]
    }
   ],
   "source": [
    "folder_test_path = \"./model_test/\"\n",
    "folder_model = os.listdir(folder_test_path)\n",
    "model_path = {}\n",
    "for folder_ in folder_model:\n",
    "    try:\n",
    "        model_path[folder_] = folder_test_path+folder_+\"/\"+os.listdir(folder_test_path+folder_)[0]\n",
    "        print(folder_,\":\",model_path[folder_])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def make_model_clf(custom_name_clf=\"\"):\n",
    "    m_ = None\n",
    "\n",
    "    if custom_name_clf == \"\": raise \"DEFINISI SALAH\"\n",
    "    \n",
    "    base = custom_name_clf.split(\"_\")[0]\n",
    "    PA_start,PA_end = (pname for pname in custom_name_clf.split(\"_\")[2].split(\"-\"))\n",
    "    \n",
    "\n",
    "\n",
    "    if base.lower()==\"cnn\":\n",
    "        ### FOR CNN MODELS ###\n",
    "\n",
    "        base_model = Sequential()\n",
    "        base_model.add(Conv2D(32, (3, 3), input_shape=(x.shape[0], x.shape[1], 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(64, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(128, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(256, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Flatten()) \n",
    "        base_model.add(Dense(1024))\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(Dropout(0.5))\n",
    "        base_model.add(Dense(64))\n",
    "        base_model.add(Activation('relu'))\n",
    "        m_ = base_model.output\n",
    "\n",
    "        ### --------------- ###\n",
    "\n",
    "    elif base.lower()==\"mobilenetv3\":\n",
    "        ### FOR MOBILENETV3 ###\n",
    "\n",
    "        import keras\n",
    "        from keras.applications import keras_applications\n",
    "        from keras_applications.mobilenet_v3 import MobileNetV3Large, MobileNetV3Small\n",
    "\n",
    "        base_model = MobileNetV3Large(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet', backend=keras.backend, layers=keras.layers, models=keras.models, utils=keras.utils) #RGB 3, grayscale 1\n",
    "\n",
    "        # RENAME LAYER TO SOLVE THE PROBLEM WHEN SAVING THE MODEL\n",
    "        for i, layer in enumerate(base_model.layers[1:]):\n",
    "            layer.name = 'layer_' + str(i) + '_' + layer.name\n",
    "\n",
    "        m_ = base_model.output\n",
    "\n",
    "        m_ = GlobalAveragePooling2D()(m_)\n",
    "        m_ = Dense(1024,activation='relu')(m_) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "        m_ = Dense(512,activation='relu')(m_) #dense layer 2\n",
    "        m_ = Dense(256,activation='relu')(m_) #dense layer 3\n",
    "        \n",
    "        ### --------------- ###\n",
    "    elif base.lower()[:-1]==\"efficientnetb\":\n",
    "        ### FOR EFFICIENTNET ###\n",
    "\n",
    "        from efficientnet.keras import EfficientNetB0,EfficientNetB1\n",
    "        from efficientnet.keras import EfficientNetB2,EfficientNetB3\n",
    "        from efficientnet.keras import EfficientNetB4,EfficientNetB5\n",
    "\n",
    "        EFMODEL = {}\n",
    "        EFMODEL['efficientnetb0'] = EfficientNetB0(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb1'] = EfficientNetB1(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb2'] = EfficientNetB2(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb3'] = EfficientNetB3(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb4'] = EfficientNetB4(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb5'] = EfficientNetB5(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "\n",
    "        base_model = EFMODEL[base.lower()]\n",
    "        # RENAME LAYER TO SOLVE THE PROBLEM WHEN SAVING THE MODEL\n",
    "        for i, layer in enumerate(base_model.layers[1:]):\n",
    "            layer.name = 'layer_' + str(i) + '_' + layer.name\n",
    "\n",
    "        m_ = base_model.output\n",
    "\n",
    "        m_ = GlobalAveragePooling2D()(m_)\n",
    "        m_ = Dense(1024,activation='relu')(m_) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "        m_ = Dense(512,activation='relu')(m_) #dense layer 2\n",
    "        m_ = Dense(256,activation='relu')(m_) #dense layer 3\n",
    "        \n",
    "        ### --------------- ###\n",
    "    else:\n",
    "        print(\"Model Definition Error\")\n",
    "    \n",
    "    outputs = Dense(1, activation = 'sigmoid')(m_) #final layer with sigmoid activation\n",
    "        \n",
    "    model = Model(inputs=base_model.input, outputs=outputs) \n",
    "    \n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss=\"binary_crossentropy\",   \n",
    "                  metrics=[\"accuracy\",f1_m,precision_m, recall_m])\n",
    "    \n",
    "    model.name = custom_name_clf\n",
    "    \n",
    "    return model, PA_start, PA_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf, PA_start, PA_end = make_model_clf(model_path[\"parrival\"].split('/')[-1])\n",
    "mscore_clf = [\"loss\",\"accuracy\",\"f1_m\",\"precision_m\", \"recall_m\"]\n",
    "model_clf.load_weights(model_path[\"parrival\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('EFFICIENTNETB5_parival_5-6_ENZ_final_all_ep50_batch32_#1_best.h5', '5', '6')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.name, PA_start, PA_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`WE SHOULD DEFINE EXACT SHARED-LAYER AND NON-SHARED-LAYER LIKE THE LOADED MODEL SOURCE! IF NOT, RESULT WILL BE DEGRADED`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Input\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "loss_function = 'mae'\n",
    "\n",
    "def make_model_reg(custom_name_reg=\"\"):\n",
    "    m_ = None\n",
    "    outputs = {}\n",
    "    \n",
    "    if custom_name_reg == \"\": raise \"DEFINISI SALAH\"\n",
    "    \n",
    "    base = custom_name_reg.split(\"_\")[0]\n",
    "    output_names = [cname for cname in custom_name_reg.split(\"_\") if cname in ['lat','long','depth','magnitude','time']]\n",
    "\n",
    "    if base.lower()==\"cnn\":\n",
    "        ### FOR CNN MODELS ###\n",
    "\n",
    "        base_model = Sequential()\n",
    "        base_model.add(Conv2D(32, (3, 3), input_shape=(x.shape[0], x.shape[1], 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(64, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(128, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Conv2D(256, (3, 3)))\n",
    "        base_model.add(BatchNormalization())\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        base_model.add(Flatten()) \n",
    "        base_model.add(Dense(1024))\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(Dense(1024))\n",
    "        base_model.add(Activation('relu'))\n",
    "        base_model.add(Dense(1024))\n",
    "        base_model.add(Activation('relu'))\n",
    "        m_ = base_model.output\n",
    "\n",
    "        ### --------------- ###\n",
    "\n",
    "    elif base.lower()==\"mobilenetv3\":\n",
    "        ### FOR MOBILENETV3 ###\n",
    "\n",
    "        import keras\n",
    "        from keras.applications import keras_applications\n",
    "        from keras_applications.mobilenet_v3 import MobileNetV3Large, MobileNetV3Small\n",
    "\n",
    "        base_model = MobileNetV3Large(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet', backend=keras.backend, layers=keras.layers, models=keras.models, utils=keras.utils) #RGB 3, grayscale 1\n",
    "\n",
    "        # RENAME LAYER TO SOLVE THE PROBLEM WHEN SAVING THE MODEL\n",
    "        for i, layer in enumerate(base_model.layers[1:]):\n",
    "            layer.name = 'layer_' + str(i) + '_' + layer.name\n",
    "\n",
    "        m_ = base_model.output\n",
    "\n",
    "        m_ = GlobalAveragePooling2D()(m_)\n",
    "        m_ = Dense(1024,activation='relu')(m_) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "#         m_ = Dense(512,activation='relu')(m_) #dense layer 2\n",
    "#         m_ = Dense(256,activation='relu')(m_) #dense layer 3\n",
    "        \n",
    "        ### --------------- ###\n",
    "    elif base.lower()[:-1]==\"efficientnetb\":\n",
    "        ### FOR EFFICIENTNET ###\n",
    "        \n",
    "        import keras\n",
    "        from efficientnet.keras import EfficientNetB0,EfficientNetB1\n",
    "        from efficientnet.keras import EfficientNetB2,EfficientNetB3\n",
    "        from efficientnet.keras import EfficientNetB4,EfficientNetB5\n",
    "\n",
    "        EFMODEL = {}\n",
    "        EFMODEL['efficientnetb0'] = EfficientNetB0(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb1'] = EfficientNetB1(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb2'] = EfficientNetB2(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb3'] = EfficientNetB3(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb4'] = EfficientNetB4(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "        EFMODEL['efficientnetb5'] = EfficientNetB5(input_shape=(x.shape[0], x.shape[1], 3), include_top=False, weights='imagenet') #RGB 3, grayscale 1\n",
    "\n",
    "        base_model = EFMODEL[base.lower()]\n",
    "        # RENAME LAYER TO SOLVE THE PROBLEM WHEN SAVING THE MODEL\n",
    "        for i, layer in enumerate(base_model.layers[1:]):\n",
    "            layer.name = 'layer_' + str(i) + '_' + layer.name\n",
    "\n",
    "        m_ = base_model.output\n",
    "\n",
    "        m_ = GlobalAveragePooling2D()(m_)\n",
    "        #kernel_initializer=keras.initializers.Orthogonal()\n",
    "        m_ = Dense(1024,activation='relu')(m_)\n",
    "        m_ = Dense(1024,activation='relu')(m_)\n",
    "        m_ = Dense(1024,activation='relu')(m_)\n",
    "\n",
    "        ### --------------- ###\n",
    "    else:\n",
    "        print(\"Model Definition Error\")\n",
    "    \n",
    "    for output_name in output_names:\n",
    "        # multioutput\n",
    "        m__ = m_\n",
    "        #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "        m_ = Dense(512,activation='relu')(m__) #dense layer 2\n",
    "        m_ = Dense(256,activation='relu')(m__) #dense layer 3\n",
    "        outputs[output_name] = Dense(1, activation = 'linear', name = output_name)(m__) #final layer with linear activation\n",
    "        \n",
    "    if len(output_names)==0: print(\"Outputs Definition Error\")\n",
    "    # single output \n",
    "    else:\n",
    "        model = Model(inputs=base_model.input, outputs=[outputs[output_name] for output_name in output_names]) \n",
    "    \n",
    "    model.compile(optimizer=Adam(1e-4),\n",
    "                  loss=loss_function,   \n",
    "                  metrics=[loss_function])\n",
    "    \n",
    "    model.name = custom_name_reg\n",
    "    \n",
    "    return model,output_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_reg = {}\n",
    "for multi_out in [_ for _ in model_path.keys() if _.startswith('multi')]:\n",
    "    try:\n",
    "        model_reg[multi_out],output_reg_multi = make_model_reg(model_path[multi_out].split('/')[-1])\n",
    "        model_reg[multi_out].load_weights(model_path[multi_out])\n",
    "    except:\n",
    "        pass\n",
    "try:   \n",
    "    for single_out in [_ for _ in model_path.keys() if _.startswith('single')]:\n",
    "        model_reg[single_out],_ = make_model_reg(model_path[single_out].split('/')[-1])\n",
    "        model_reg[single_out].load_weights(model_path[single_out])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['multi_lat_long_depth_magnitude_time'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EFFICIENTNETB5_multioutput_lat_long_depth_magnitude_time_5s-6s_mae_B_4_1x1024orto_paral512-256_best.h5'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model reg\n",
    "print(model_reg.keys())\n",
    "model_reg['multi_lat_long_depth_magnitude_time'].name\n",
    "\n",
    "### WE SHOULD DEFINE EXACT SHARED-LAYER AND NON-SHARED-LAYER LIKE THE LOADED MODEL SOURCE, IF NOT, RESULT WILL BE DEGRADED "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EFFICIENTNETB5_multioutput_lat_long_depth_magnitude_time_5s-6s_mae_B_4_1x1024orto_paral512-256_best.h5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           (None, 256, 192, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "layer_0_stem_conv (Conv2D)      (None, 128, 96, 48)  1296        input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_1_stem_bn (BatchNormaliza (None, 128, 96, 48)  192         layer_0_stem_conv[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "layer_2_stem_activation (Activa (None, 128, 96, 48)  0           layer_1_stem_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_3_block1a_dwconv (Depthwi (None, 128, 96, 48)  432         layer_2_stem_activation[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_4_block1a_bn (BatchNormal (None, 128, 96, 48)  192         layer_3_block1a_dwconv[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_5_block1a_activation (Act (None, 128, 96, 48)  0           layer_4_block1a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_6_block1a_se_squeeze (Glo (None, 48)           0           layer_5_block1a_activation[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_7_block1a_se_reshape (Res (None, 1, 1, 48)     0           layer_6_block1a_se_squeeze[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_8_block1a_se_reduce (Conv (None, 1, 1, 12)     588         layer_7_block1a_se_reshape[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_9_block1a_se_expand (Conv (None, 1, 1, 48)     624         layer_8_block1a_se_reduce[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_10_block1a_se_excite (Mul (None, 128, 96, 48)  0           layer_5_block1a_activation[0][0] \n",
      "                                                                 layer_9_block1a_se_expand[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_11_block1a_project_conv ( (None, 128, 96, 24)  1152        layer_10_block1a_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_12_block1a_project_bn (Ba (None, 128, 96, 24)  96          layer_11_block1a_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_13_block1b_dwconv (Depthw (None, 128, 96, 24)  216         layer_12_block1a_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_14_block1b_bn (BatchNorma (None, 128, 96, 24)  96          layer_13_block1b_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_15_block1b_activation (Ac (None, 128, 96, 24)  0           layer_14_block1b_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_16_block1b_se_squeeze (Gl (None, 24)           0           layer_15_block1b_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_17_block1b_se_reshape (Re (None, 1, 1, 24)     0           layer_16_block1b_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_18_block1b_se_reduce (Con (None, 1, 1, 6)      150         layer_17_block1b_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_19_block1b_se_expand (Con (None, 1, 1, 24)     168         layer_18_block1b_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_20_block1b_se_excite (Mul (None, 128, 96, 24)  0           layer_15_block1b_activation[0][0]\n",
      "                                                                 layer_19_block1b_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_21_block1b_project_conv ( (None, 128, 96, 24)  576         layer_20_block1b_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_22_block1b_project_bn (Ba (None, 128, 96, 24)  96          layer_21_block1b_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_23_block1b_drop (FixedDro (None, 128, 96, 24)  0           layer_22_block1b_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_24_block1b_add (Add)      (None, 128, 96, 24)  0           layer_23_block1b_drop[0][0]      \n",
      "                                                                 layer_12_block1a_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_25_block1c_dwconv (Depthw (None, 128, 96, 24)  216         layer_24_block1b_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_26_block1c_bn (BatchNorma (None, 128, 96, 24)  96          layer_25_block1c_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_27_block1c_activation (Ac (None, 128, 96, 24)  0           layer_26_block1c_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_28_block1c_se_squeeze (Gl (None, 24)           0           layer_27_block1c_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_29_block1c_se_reshape (Re (None, 1, 1, 24)     0           layer_28_block1c_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_30_block1c_se_reduce (Con (None, 1, 1, 6)      150         layer_29_block1c_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_31_block1c_se_expand (Con (None, 1, 1, 24)     168         layer_30_block1c_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_32_block1c_se_excite (Mul (None, 128, 96, 24)  0           layer_27_block1c_activation[0][0]\n",
      "                                                                 layer_31_block1c_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_33_block1c_project_conv ( (None, 128, 96, 24)  576         layer_32_block1c_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_34_block1c_project_bn (Ba (None, 128, 96, 24)  96          layer_33_block1c_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_35_block1c_drop (FixedDro (None, 128, 96, 24)  0           layer_34_block1c_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_36_block1c_add (Add)      (None, 128, 96, 24)  0           layer_35_block1c_drop[0][0]      \n",
      "                                                                 layer_24_block1b_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_37_block2a_expand_conv (C (None, 128, 96, 144) 3456        layer_36_block1c_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_38_block2a_expand_bn (Bat (None, 128, 96, 144) 576         layer_37_block2a_expand_conv[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_39_block2a_expand_activat (None, 128, 96, 144) 0           layer_38_block2a_expand_bn[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_40_block2a_dwconv (Depthw (None, 64, 48, 144)  1296        layer_39_block2a_expand_activatio\n",
      "__________________________________________________________________________________________________\n",
      "layer_41_block2a_bn (BatchNorma (None, 64, 48, 144)  576         layer_40_block2a_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_42_block2a_activation (Ac (None, 64, 48, 144)  0           layer_41_block2a_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_43_block2a_se_squeeze (Gl (None, 144)          0           layer_42_block2a_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_44_block2a_se_reshape (Re (None, 1, 1, 144)    0           layer_43_block2a_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_45_block2a_se_reduce (Con (None, 1, 1, 6)      870         layer_44_block2a_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_46_block2a_se_expand (Con (None, 1, 1, 144)    1008        layer_45_block2a_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_47_block2a_se_excite (Mul (None, 64, 48, 144)  0           layer_42_block2a_activation[0][0]\n",
      "                                                                 layer_46_block2a_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_48_block2a_project_conv ( (None, 64, 48, 40)   5760        layer_47_block2a_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_49_block2a_project_bn (Ba (None, 64, 48, 40)   160         layer_48_block2a_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_50_block2b_expand_conv (C (None, 64, 48, 240)  9600        layer_49_block2a_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_51_block2b_expand_bn (Bat (None, 64, 48, 240)  960         layer_50_block2b_expand_conv[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_52_block2b_expand_activat (None, 64, 48, 240)  0           layer_51_block2b_expand_bn[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_53_block2b_dwconv (Depthw (None, 64, 48, 240)  2160        layer_52_block2b_expand_activatio\n",
      "__________________________________________________________________________________________________\n",
      "layer_54_block2b_bn (BatchNorma (None, 64, 48, 240)  960         layer_53_block2b_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_55_block2b_activation (Ac (None, 64, 48, 240)  0           layer_54_block2b_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_56_block2b_se_squeeze (Gl (None, 240)          0           layer_55_block2b_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_57_block2b_se_reshape (Re (None, 1, 1, 240)    0           layer_56_block2b_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_58_block2b_se_reduce (Con (None, 1, 1, 10)     2410        layer_57_block2b_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_59_block2b_se_expand (Con (None, 1, 1, 240)    2640        layer_58_block2b_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_60_block2b_se_excite (Mul (None, 64, 48, 240)  0           layer_55_block2b_activation[0][0]\n",
      "                                                                 layer_59_block2b_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_61_block2b_project_conv ( (None, 64, 48, 40)   9600        layer_60_block2b_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_62_block2b_project_bn (Ba (None, 64, 48, 40)   160         layer_61_block2b_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_63_block2b_drop (FixedDro (None, 64, 48, 40)   0           layer_62_block2b_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_64_block2b_add (Add)      (None, 64, 48, 40)   0           layer_63_block2b_drop[0][0]      \n",
      "                                                                 layer_49_block2a_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_65_block2c_expand_conv (C (None, 64, 48, 240)  9600        layer_64_block2b_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_66_block2c_expand_bn (Bat (None, 64, 48, 240)  960         layer_65_block2c_expand_conv[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_67_block2c_expand_activat (None, 64, 48, 240)  0           layer_66_block2c_expand_bn[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_68_block2c_dwconv (Depthw (None, 64, 48, 240)  2160        layer_67_block2c_expand_activatio\n",
      "__________________________________________________________________________________________________\n",
      "layer_69_block2c_bn (BatchNorma (None, 64, 48, 240)  960         layer_68_block2c_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_70_block2c_activation (Ac (None, 64, 48, 240)  0           layer_69_block2c_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_71_block2c_se_squeeze (Gl (None, 240)          0           layer_70_block2c_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_72_block2c_se_reshape (Re (None, 1, 1, 240)    0           layer_71_block2c_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_73_block2c_se_reduce (Con (None, 1, 1, 10)     2410        layer_72_block2c_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_74_block2c_se_expand (Con (None, 1, 1, 240)    2640        layer_73_block2c_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_75_block2c_se_excite (Mul (None, 64, 48, 240)  0           layer_70_block2c_activation[0][0]\n",
      "                                                                 layer_74_block2c_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_76_block2c_project_conv ( (None, 64, 48, 40)   9600        layer_75_block2c_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_77_block2c_project_bn (Ba (None, 64, 48, 40)   160         layer_76_block2c_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_78_block2c_drop (FixedDro (None, 64, 48, 40)   0           layer_77_block2c_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_79_block2c_add (Add)      (None, 64, 48, 40)   0           layer_78_block2c_drop[0][0]      \n",
      "                                                                 layer_64_block2b_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_80_block2d_expand_conv (C (None, 64, 48, 240)  9600        layer_79_block2c_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_81_block2d_expand_bn (Bat (None, 64, 48, 240)  960         layer_80_block2d_expand_conv[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_82_block2d_expand_activat (None, 64, 48, 240)  0           layer_81_block2d_expand_bn[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_83_block2d_dwconv (Depthw (None, 64, 48, 240)  2160        layer_82_block2d_expand_activatio\n",
      "__________________________________________________________________________________________________\n",
      "layer_84_block2d_bn (BatchNorma (None, 64, 48, 240)  960         layer_83_block2d_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_85_block2d_activation (Ac (None, 64, 48, 240)  0           layer_84_block2d_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_86_block2d_se_squeeze (Gl (None, 240)          0           layer_85_block2d_activation[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_87_block2d_se_reshape (Re (None, 1, 1, 240)    0           layer_86_block2d_se_squeeze[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_88_block2d_se_reduce (Con (None, 1, 1, 10)     2410        layer_87_block2d_se_reshape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_89_block2d_se_expand (Con (None, 1, 1, 240)    2640        layer_88_block2d_se_reduce[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_90_block2d_se_excite (Mul (None, 64, 48, 240)  0           layer_85_block2d_activation[0][0]\n",
      "                                                                 layer_89_block2d_se_expand[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_91_block2d_project_conv ( (None, 64, 48, 40)   9600        layer_90_block2d_se_excite[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_92_block2d_project_bn (Ba (None, 64, 48, 40)   160         layer_91_block2d_project_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_93_block2d_drop (FixedDro (None, 64, 48, 40)   0           layer_92_block2d_project_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_94_block2d_add (Add)      (None, 64, 48, 40)   0           layer_93_block2d_drop[0][0]      \n",
      "                                                                 layer_79_block2c_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_95_block2e_expand_conv (C (None, 64, 48, 240)  9600        layer_94_block2d_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_96_block2e_expand_bn (Bat (None, 64, 48, 240)  960         layer_95_block2e_expand_conv[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_97_block2e_expand_activat (None, 64, 48, 240)  0           layer_96_block2e_expand_bn[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "layer_98_block2e_dwconv (Depthw (None, 64, 48, 240)  2160        layer_97_block2e_expand_activatio\n",
      "__________________________________________________________________________________________________\n",
      "layer_99_block2e_bn (BatchNorma (None, 64, 48, 240)  960         layer_98_block2e_dwconv[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "layer_100_block2e_activation (A (None, 64, 48, 240)  0           layer_99_block2e_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_101_block2e_se_squeeze (G (None, 240)          0           layer_100_block2e_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_102_block2e_se_reshape (R (None, 1, 1, 240)    0           layer_101_block2e_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_103_block2e_se_reduce (Co (None, 1, 1, 10)     2410        layer_102_block2e_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_104_block2e_se_expand (Co (None, 1, 1, 240)    2640        layer_103_block2e_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_105_block2e_se_excite (Mu (None, 64, 48, 240)  0           layer_100_block2e_activation[0][0\n",
      "                                                                 layer_104_block2e_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_106_block2e_project_conv  (None, 64, 48, 40)   9600        layer_105_block2e_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_107_block2e_project_bn (B (None, 64, 48, 40)   160         layer_106_block2e_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_108_block2e_drop (FixedDr (None, 64, 48, 40)   0           layer_107_block2e_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_109_block2e_add (Add)     (None, 64, 48, 40)   0           layer_108_block2e_drop[0][0]     \n",
      "                                                                 layer_94_block2d_add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_110_block3a_expand_conv ( (None, 64, 48, 240)  9600        layer_109_block2e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_111_block3a_expand_bn (Ba (None, 64, 48, 240)  960         layer_110_block3a_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_112_block3a_expand_activa (None, 64, 48, 240)  0           layer_111_block3a_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_113_block3a_dwconv (Depth (None, 32, 24, 240)  6000        layer_112_block3a_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_114_block3a_bn (BatchNorm (None, 32, 24, 240)  960         layer_113_block3a_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_115_block3a_activation (A (None, 32, 24, 240)  0           layer_114_block3a_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_116_block3a_se_squeeze (G (None, 240)          0           layer_115_block3a_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_117_block3a_se_reshape (R (None, 1, 1, 240)    0           layer_116_block3a_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_118_block3a_se_reduce (Co (None, 1, 1, 10)     2410        layer_117_block3a_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_119_block3a_se_expand (Co (None, 1, 1, 240)    2640        layer_118_block3a_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_120_block3a_se_excite (Mu (None, 32, 24, 240)  0           layer_115_block3a_activation[0][0\n",
      "                                                                 layer_119_block3a_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_121_block3a_project_conv  (None, 32, 24, 64)   15360       layer_120_block3a_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_122_block3a_project_bn (B (None, 32, 24, 64)   256         layer_121_block3a_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_123_block3b_expand_conv ( (None, 32, 24, 384)  24576       layer_122_block3a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_124_block3b_expand_bn (Ba (None, 32, 24, 384)  1536        layer_123_block3b_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_125_block3b_expand_activa (None, 32, 24, 384)  0           layer_124_block3b_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_126_block3b_dwconv (Depth (None, 32, 24, 384)  9600        layer_125_block3b_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_127_block3b_bn (BatchNorm (None, 32, 24, 384)  1536        layer_126_block3b_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_128_block3b_activation (A (None, 32, 24, 384)  0           layer_127_block3b_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_129_block3b_se_squeeze (G (None, 384)          0           layer_128_block3b_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_130_block3b_se_reshape (R (None, 1, 1, 384)    0           layer_129_block3b_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_131_block3b_se_reduce (Co (None, 1, 1, 16)     6160        layer_130_block3b_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_132_block3b_se_expand (Co (None, 1, 1, 384)    6528        layer_131_block3b_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_133_block3b_se_excite (Mu (None, 32, 24, 384)  0           layer_128_block3b_activation[0][0\n",
      "                                                                 layer_132_block3b_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_134_block3b_project_conv  (None, 32, 24, 64)   24576       layer_133_block3b_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_135_block3b_project_bn (B (None, 32, 24, 64)   256         layer_134_block3b_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_136_block3b_drop (FixedDr (None, 32, 24, 64)   0           layer_135_block3b_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_137_block3b_add (Add)     (None, 32, 24, 64)   0           layer_136_block3b_drop[0][0]     \n",
      "                                                                 layer_122_block3a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_138_block3c_expand_conv ( (None, 32, 24, 384)  24576       layer_137_block3b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_139_block3c_expand_bn (Ba (None, 32, 24, 384)  1536        layer_138_block3c_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_140_block3c_expand_activa (None, 32, 24, 384)  0           layer_139_block3c_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_141_block3c_dwconv (Depth (None, 32, 24, 384)  9600        layer_140_block3c_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_142_block3c_bn (BatchNorm (None, 32, 24, 384)  1536        layer_141_block3c_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_143_block3c_activation (A (None, 32, 24, 384)  0           layer_142_block3c_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_144_block3c_se_squeeze (G (None, 384)          0           layer_143_block3c_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_145_block3c_se_reshape (R (None, 1, 1, 384)    0           layer_144_block3c_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_146_block3c_se_reduce (Co (None, 1, 1, 16)     6160        layer_145_block3c_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_147_block3c_se_expand (Co (None, 1, 1, 384)    6528        layer_146_block3c_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_148_block3c_se_excite (Mu (None, 32, 24, 384)  0           layer_143_block3c_activation[0][0\n",
      "                                                                 layer_147_block3c_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_149_block3c_project_conv  (None, 32, 24, 64)   24576       layer_148_block3c_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_150_block3c_project_bn (B (None, 32, 24, 64)   256         layer_149_block3c_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_151_block3c_drop (FixedDr (None, 32, 24, 64)   0           layer_150_block3c_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_152_block3c_add (Add)     (None, 32, 24, 64)   0           layer_151_block3c_drop[0][0]     \n",
      "                                                                 layer_137_block3b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_153_block3d_expand_conv ( (None, 32, 24, 384)  24576       layer_152_block3c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_154_block3d_expand_bn (Ba (None, 32, 24, 384)  1536        layer_153_block3d_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_155_block3d_expand_activa (None, 32, 24, 384)  0           layer_154_block3d_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_156_block3d_dwconv (Depth (None, 32, 24, 384)  9600        layer_155_block3d_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_157_block3d_bn (BatchNorm (None, 32, 24, 384)  1536        layer_156_block3d_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_158_block3d_activation (A (None, 32, 24, 384)  0           layer_157_block3d_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_159_block3d_se_squeeze (G (None, 384)          0           layer_158_block3d_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_160_block3d_se_reshape (R (None, 1, 1, 384)    0           layer_159_block3d_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_161_block3d_se_reduce (Co (None, 1, 1, 16)     6160        layer_160_block3d_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_162_block3d_se_expand (Co (None, 1, 1, 384)    6528        layer_161_block3d_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_163_block3d_se_excite (Mu (None, 32, 24, 384)  0           layer_158_block3d_activation[0][0\n",
      "                                                                 layer_162_block3d_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_164_block3d_project_conv  (None, 32, 24, 64)   24576       layer_163_block3d_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_165_block3d_project_bn (B (None, 32, 24, 64)   256         layer_164_block3d_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_166_block3d_drop (FixedDr (None, 32, 24, 64)   0           layer_165_block3d_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_167_block3d_add (Add)     (None, 32, 24, 64)   0           layer_166_block3d_drop[0][0]     \n",
      "                                                                 layer_152_block3c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_168_block3e_expand_conv ( (None, 32, 24, 384)  24576       layer_167_block3d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_169_block3e_expand_bn (Ba (None, 32, 24, 384)  1536        layer_168_block3e_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_170_block3e_expand_activa (None, 32, 24, 384)  0           layer_169_block3e_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_171_block3e_dwconv (Depth (None, 32, 24, 384)  9600        layer_170_block3e_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_172_block3e_bn (BatchNorm (None, 32, 24, 384)  1536        layer_171_block3e_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_173_block3e_activation (A (None, 32, 24, 384)  0           layer_172_block3e_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_174_block3e_se_squeeze (G (None, 384)          0           layer_173_block3e_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_175_block3e_se_reshape (R (None, 1, 1, 384)    0           layer_174_block3e_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_176_block3e_se_reduce (Co (None, 1, 1, 16)     6160        layer_175_block3e_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_177_block3e_se_expand (Co (None, 1, 1, 384)    6528        layer_176_block3e_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_178_block3e_se_excite (Mu (None, 32, 24, 384)  0           layer_173_block3e_activation[0][0\n",
      "                                                                 layer_177_block3e_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_179_block3e_project_conv  (None, 32, 24, 64)   24576       layer_178_block3e_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_180_block3e_project_bn (B (None, 32, 24, 64)   256         layer_179_block3e_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_181_block3e_drop (FixedDr (None, 32, 24, 64)   0           layer_180_block3e_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_182_block3e_add (Add)     (None, 32, 24, 64)   0           layer_181_block3e_drop[0][0]     \n",
      "                                                                 layer_167_block3d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_183_block4a_expand_conv ( (None, 32, 24, 384)  24576       layer_182_block3e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_184_block4a_expand_bn (Ba (None, 32, 24, 384)  1536        layer_183_block4a_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_185_block4a_expand_activa (None, 32, 24, 384)  0           layer_184_block4a_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_186_block4a_dwconv (Depth (None, 16, 12, 384)  3456        layer_185_block4a_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_187_block4a_bn (BatchNorm (None, 16, 12, 384)  1536        layer_186_block4a_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_188_block4a_activation (A (None, 16, 12, 384)  0           layer_187_block4a_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_189_block4a_se_squeeze (G (None, 384)          0           layer_188_block4a_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_190_block4a_se_reshape (R (None, 1, 1, 384)    0           layer_189_block4a_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_191_block4a_se_reduce (Co (None, 1, 1, 16)     6160        layer_190_block4a_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_192_block4a_se_expand (Co (None, 1, 1, 384)    6528        layer_191_block4a_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_193_block4a_se_excite (Mu (None, 16, 12, 384)  0           layer_188_block4a_activation[0][0\n",
      "                                                                 layer_192_block4a_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_194_block4a_project_conv  (None, 16, 12, 128)  49152       layer_193_block4a_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_195_block4a_project_bn (B (None, 16, 12, 128)  512         layer_194_block4a_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_196_block4b_expand_conv ( (None, 16, 12, 768)  98304       layer_195_block4a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_197_block4b_expand_bn (Ba (None, 16, 12, 768)  3072        layer_196_block4b_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_198_block4b_expand_activa (None, 16, 12, 768)  0           layer_197_block4b_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_199_block4b_dwconv (Depth (None, 16, 12, 768)  6912        layer_198_block4b_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_200_block4b_bn (BatchNorm (None, 16, 12, 768)  3072        layer_199_block4b_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_201_block4b_activation (A (None, 16, 12, 768)  0           layer_200_block4b_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_202_block4b_se_squeeze (G (None, 768)          0           layer_201_block4b_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_203_block4b_se_reshape (R (None, 1, 1, 768)    0           layer_202_block4b_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_204_block4b_se_reduce (Co (None, 1, 1, 32)     24608       layer_203_block4b_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_205_block4b_se_expand (Co (None, 1, 1, 768)    25344       layer_204_block4b_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_206_block4b_se_excite (Mu (None, 16, 12, 768)  0           layer_201_block4b_activation[0][0\n",
      "                                                                 layer_205_block4b_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_207_block4b_project_conv  (None, 16, 12, 128)  98304       layer_206_block4b_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_208_block4b_project_bn (B (None, 16, 12, 128)  512         layer_207_block4b_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_209_block4b_drop (FixedDr (None, 16, 12, 128)  0           layer_208_block4b_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_210_block4b_add (Add)     (None, 16, 12, 128)  0           layer_209_block4b_drop[0][0]     \n",
      "                                                                 layer_195_block4a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_211_block4c_expand_conv ( (None, 16, 12, 768)  98304       layer_210_block4b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_212_block4c_expand_bn (Ba (None, 16, 12, 768)  3072        layer_211_block4c_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_213_block4c_expand_activa (None, 16, 12, 768)  0           layer_212_block4c_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_214_block4c_dwconv (Depth (None, 16, 12, 768)  6912        layer_213_block4c_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_215_block4c_bn (BatchNorm (None, 16, 12, 768)  3072        layer_214_block4c_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_216_block4c_activation (A (None, 16, 12, 768)  0           layer_215_block4c_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_217_block4c_se_squeeze (G (None, 768)          0           layer_216_block4c_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_218_block4c_se_reshape (R (None, 1, 1, 768)    0           layer_217_block4c_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_219_block4c_se_reduce (Co (None, 1, 1, 32)     24608       layer_218_block4c_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_220_block4c_se_expand (Co (None, 1, 1, 768)    25344       layer_219_block4c_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_221_block4c_se_excite (Mu (None, 16, 12, 768)  0           layer_216_block4c_activation[0][0\n",
      "                                                                 layer_220_block4c_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_222_block4c_project_conv  (None, 16, 12, 128)  98304       layer_221_block4c_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_223_block4c_project_bn (B (None, 16, 12, 128)  512         layer_222_block4c_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_224_block4c_drop (FixedDr (None, 16, 12, 128)  0           layer_223_block4c_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_225_block4c_add (Add)     (None, 16, 12, 128)  0           layer_224_block4c_drop[0][0]     \n",
      "                                                                 layer_210_block4b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_226_block4d_expand_conv ( (None, 16, 12, 768)  98304       layer_225_block4c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_227_block4d_expand_bn (Ba (None, 16, 12, 768)  3072        layer_226_block4d_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_228_block4d_expand_activa (None, 16, 12, 768)  0           layer_227_block4d_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_229_block4d_dwconv (Depth (None, 16, 12, 768)  6912        layer_228_block4d_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_230_block4d_bn (BatchNorm (None, 16, 12, 768)  3072        layer_229_block4d_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_231_block4d_activation (A (None, 16, 12, 768)  0           layer_230_block4d_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_232_block4d_se_squeeze (G (None, 768)          0           layer_231_block4d_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_233_block4d_se_reshape (R (None, 1, 1, 768)    0           layer_232_block4d_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_234_block4d_se_reduce (Co (None, 1, 1, 32)     24608       layer_233_block4d_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_235_block4d_se_expand (Co (None, 1, 1, 768)    25344       layer_234_block4d_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_236_block4d_se_excite (Mu (None, 16, 12, 768)  0           layer_231_block4d_activation[0][0\n",
      "                                                                 layer_235_block4d_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_237_block4d_project_conv  (None, 16, 12, 128)  98304       layer_236_block4d_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_238_block4d_project_bn (B (None, 16, 12, 128)  512         layer_237_block4d_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_239_block4d_drop (FixedDr (None, 16, 12, 128)  0           layer_238_block4d_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_240_block4d_add (Add)     (None, 16, 12, 128)  0           layer_239_block4d_drop[0][0]     \n",
      "                                                                 layer_225_block4c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_241_block4e_expand_conv ( (None, 16, 12, 768)  98304       layer_240_block4d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_242_block4e_expand_bn (Ba (None, 16, 12, 768)  3072        layer_241_block4e_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_243_block4e_expand_activa (None, 16, 12, 768)  0           layer_242_block4e_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_244_block4e_dwconv (Depth (None, 16, 12, 768)  6912        layer_243_block4e_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_245_block4e_bn (BatchNorm (None, 16, 12, 768)  3072        layer_244_block4e_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_246_block4e_activation (A (None, 16, 12, 768)  0           layer_245_block4e_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_247_block4e_se_squeeze (G (None, 768)          0           layer_246_block4e_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_248_block4e_se_reshape (R (None, 1, 1, 768)    0           layer_247_block4e_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_249_block4e_se_reduce (Co (None, 1, 1, 32)     24608       layer_248_block4e_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_250_block4e_se_expand (Co (None, 1, 1, 768)    25344       layer_249_block4e_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_251_block4e_se_excite (Mu (None, 16, 12, 768)  0           layer_246_block4e_activation[0][0\n",
      "                                                                 layer_250_block4e_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_252_block4e_project_conv  (None, 16, 12, 128)  98304       layer_251_block4e_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_253_block4e_project_bn (B (None, 16, 12, 128)  512         layer_252_block4e_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_254_block4e_drop (FixedDr (None, 16, 12, 128)  0           layer_253_block4e_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_255_block4e_add (Add)     (None, 16, 12, 128)  0           layer_254_block4e_drop[0][0]     \n",
      "                                                                 layer_240_block4d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_256_block4f_expand_conv ( (None, 16, 12, 768)  98304       layer_255_block4e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_257_block4f_expand_bn (Ba (None, 16, 12, 768)  3072        layer_256_block4f_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_258_block4f_expand_activa (None, 16, 12, 768)  0           layer_257_block4f_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_259_block4f_dwconv (Depth (None, 16, 12, 768)  6912        layer_258_block4f_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_260_block4f_bn (BatchNorm (None, 16, 12, 768)  3072        layer_259_block4f_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_261_block4f_activation (A (None, 16, 12, 768)  0           layer_260_block4f_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_262_block4f_se_squeeze (G (None, 768)          0           layer_261_block4f_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_263_block4f_se_reshape (R (None, 1, 1, 768)    0           layer_262_block4f_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_264_block4f_se_reduce (Co (None, 1, 1, 32)     24608       layer_263_block4f_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_265_block4f_se_expand (Co (None, 1, 1, 768)    25344       layer_264_block4f_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_266_block4f_se_excite (Mu (None, 16, 12, 768)  0           layer_261_block4f_activation[0][0\n",
      "                                                                 layer_265_block4f_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_267_block4f_project_conv  (None, 16, 12, 128)  98304       layer_266_block4f_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_268_block4f_project_bn (B (None, 16, 12, 128)  512         layer_267_block4f_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_269_block4f_drop (FixedDr (None, 16, 12, 128)  0           layer_268_block4f_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_270_block4f_add (Add)     (None, 16, 12, 128)  0           layer_269_block4f_drop[0][0]     \n",
      "                                                                 layer_255_block4e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_271_block4g_expand_conv ( (None, 16, 12, 768)  98304       layer_270_block4f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_272_block4g_expand_bn (Ba (None, 16, 12, 768)  3072        layer_271_block4g_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_273_block4g_expand_activa (None, 16, 12, 768)  0           layer_272_block4g_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_274_block4g_dwconv (Depth (None, 16, 12, 768)  6912        layer_273_block4g_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_275_block4g_bn (BatchNorm (None, 16, 12, 768)  3072        layer_274_block4g_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_276_block4g_activation (A (None, 16, 12, 768)  0           layer_275_block4g_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_277_block4g_se_squeeze (G (None, 768)          0           layer_276_block4g_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_278_block4g_se_reshape (R (None, 1, 1, 768)    0           layer_277_block4g_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_279_block4g_se_reduce (Co (None, 1, 1, 32)     24608       layer_278_block4g_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_280_block4g_se_expand (Co (None, 1, 1, 768)    25344       layer_279_block4g_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_281_block4g_se_excite (Mu (None, 16, 12, 768)  0           layer_276_block4g_activation[0][0\n",
      "                                                                 layer_280_block4g_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_282_block4g_project_conv  (None, 16, 12, 128)  98304       layer_281_block4g_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_283_block4g_project_bn (B (None, 16, 12, 128)  512         layer_282_block4g_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_284_block4g_drop (FixedDr (None, 16, 12, 128)  0           layer_283_block4g_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_285_block4g_add (Add)     (None, 16, 12, 128)  0           layer_284_block4g_drop[0][0]     \n",
      "                                                                 layer_270_block4f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_286_block5a_expand_conv ( (None, 16, 12, 768)  98304       layer_285_block4g_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_287_block5a_expand_bn (Ba (None, 16, 12, 768)  3072        layer_286_block5a_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_288_block5a_expand_activa (None, 16, 12, 768)  0           layer_287_block5a_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_289_block5a_dwconv (Depth (None, 16, 12, 768)  19200       layer_288_block5a_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_290_block5a_bn (BatchNorm (None, 16, 12, 768)  3072        layer_289_block5a_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_291_block5a_activation (A (None, 16, 12, 768)  0           layer_290_block5a_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_292_block5a_se_squeeze (G (None, 768)          0           layer_291_block5a_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_293_block5a_se_reshape (R (None, 1, 1, 768)    0           layer_292_block5a_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_294_block5a_se_reduce (Co (None, 1, 1, 32)     24608       layer_293_block5a_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_295_block5a_se_expand (Co (None, 1, 1, 768)    25344       layer_294_block5a_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_296_block5a_se_excite (Mu (None, 16, 12, 768)  0           layer_291_block5a_activation[0][0\n",
      "                                                                 layer_295_block5a_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_297_block5a_project_conv  (None, 16, 12, 176)  135168      layer_296_block5a_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_298_block5a_project_bn (B (None, 16, 12, 176)  704         layer_297_block5a_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_299_block5b_expand_conv ( (None, 16, 12, 1056) 185856      layer_298_block5a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_300_block5b_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_299_block5b_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_301_block5b_expand_activa (None, 16, 12, 1056) 0           layer_300_block5b_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_302_block5b_dwconv (Depth (None, 16, 12, 1056) 26400       layer_301_block5b_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_303_block5b_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_302_block5b_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_304_block5b_activation (A (None, 16, 12, 1056) 0           layer_303_block5b_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_305_block5b_se_squeeze (G (None, 1056)         0           layer_304_block5b_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_306_block5b_se_reshape (R (None, 1, 1, 1056)   0           layer_305_block5b_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_307_block5b_se_reduce (Co (None, 1, 1, 44)     46508       layer_306_block5b_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_308_block5b_se_expand (Co (None, 1, 1, 1056)   47520       layer_307_block5b_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_309_block5b_se_excite (Mu (None, 16, 12, 1056) 0           layer_304_block5b_activation[0][0\n",
      "                                                                 layer_308_block5b_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_310_block5b_project_conv  (None, 16, 12, 176)  185856      layer_309_block5b_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_311_block5b_project_bn (B (None, 16, 12, 176)  704         layer_310_block5b_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_312_block5b_drop (FixedDr (None, 16, 12, 176)  0           layer_311_block5b_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_313_block5b_add (Add)     (None, 16, 12, 176)  0           layer_312_block5b_drop[0][0]     \n",
      "                                                                 layer_298_block5a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_314_block5c_expand_conv ( (None, 16, 12, 1056) 185856      layer_313_block5b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_315_block5c_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_314_block5c_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_316_block5c_expand_activa (None, 16, 12, 1056) 0           layer_315_block5c_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_317_block5c_dwconv (Depth (None, 16, 12, 1056) 26400       layer_316_block5c_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_318_block5c_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_317_block5c_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_319_block5c_activation (A (None, 16, 12, 1056) 0           layer_318_block5c_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_320_block5c_se_squeeze (G (None, 1056)         0           layer_319_block5c_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_321_block5c_se_reshape (R (None, 1, 1, 1056)   0           layer_320_block5c_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_322_block5c_se_reduce (Co (None, 1, 1, 44)     46508       layer_321_block5c_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_323_block5c_se_expand (Co (None, 1, 1, 1056)   47520       layer_322_block5c_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_324_block5c_se_excite (Mu (None, 16, 12, 1056) 0           layer_319_block5c_activation[0][0\n",
      "                                                                 layer_323_block5c_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_325_block5c_project_conv  (None, 16, 12, 176)  185856      layer_324_block5c_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_326_block5c_project_bn (B (None, 16, 12, 176)  704         layer_325_block5c_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_327_block5c_drop (FixedDr (None, 16, 12, 176)  0           layer_326_block5c_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_328_block5c_add (Add)     (None, 16, 12, 176)  0           layer_327_block5c_drop[0][0]     \n",
      "                                                                 layer_313_block5b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_329_block5d_expand_conv ( (None, 16, 12, 1056) 185856      layer_328_block5c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_330_block5d_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_329_block5d_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_331_block5d_expand_activa (None, 16, 12, 1056) 0           layer_330_block5d_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_332_block5d_dwconv (Depth (None, 16, 12, 1056) 26400       layer_331_block5d_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_333_block5d_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_332_block5d_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_334_block5d_activation (A (None, 16, 12, 1056) 0           layer_333_block5d_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_335_block5d_se_squeeze (G (None, 1056)         0           layer_334_block5d_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_336_block5d_se_reshape (R (None, 1, 1, 1056)   0           layer_335_block5d_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_337_block5d_se_reduce (Co (None, 1, 1, 44)     46508       layer_336_block5d_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_338_block5d_se_expand (Co (None, 1, 1, 1056)   47520       layer_337_block5d_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_339_block5d_se_excite (Mu (None, 16, 12, 1056) 0           layer_334_block5d_activation[0][0\n",
      "                                                                 layer_338_block5d_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_340_block5d_project_conv  (None, 16, 12, 176)  185856      layer_339_block5d_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_341_block5d_project_bn (B (None, 16, 12, 176)  704         layer_340_block5d_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_342_block5d_drop (FixedDr (None, 16, 12, 176)  0           layer_341_block5d_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_343_block5d_add (Add)     (None, 16, 12, 176)  0           layer_342_block5d_drop[0][0]     \n",
      "                                                                 layer_328_block5c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_344_block5e_expand_conv ( (None, 16, 12, 1056) 185856      layer_343_block5d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_345_block5e_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_344_block5e_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_346_block5e_expand_activa (None, 16, 12, 1056) 0           layer_345_block5e_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_347_block5e_dwconv (Depth (None, 16, 12, 1056) 26400       layer_346_block5e_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_348_block5e_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_347_block5e_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_349_block5e_activation (A (None, 16, 12, 1056) 0           layer_348_block5e_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_350_block5e_se_squeeze (G (None, 1056)         0           layer_349_block5e_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_351_block5e_se_reshape (R (None, 1, 1, 1056)   0           layer_350_block5e_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_352_block5e_se_reduce (Co (None, 1, 1, 44)     46508       layer_351_block5e_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_353_block5e_se_expand (Co (None, 1, 1, 1056)   47520       layer_352_block5e_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_354_block5e_se_excite (Mu (None, 16, 12, 1056) 0           layer_349_block5e_activation[0][0\n",
      "                                                                 layer_353_block5e_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_355_block5e_project_conv  (None, 16, 12, 176)  185856      layer_354_block5e_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_356_block5e_project_bn (B (None, 16, 12, 176)  704         layer_355_block5e_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_357_block5e_drop (FixedDr (None, 16, 12, 176)  0           layer_356_block5e_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_358_block5e_add (Add)     (None, 16, 12, 176)  0           layer_357_block5e_drop[0][0]     \n",
      "                                                                 layer_343_block5d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_359_block5f_expand_conv ( (None, 16, 12, 1056) 185856      layer_358_block5e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_360_block5f_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_359_block5f_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_361_block5f_expand_activa (None, 16, 12, 1056) 0           layer_360_block5f_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_362_block5f_dwconv (Depth (None, 16, 12, 1056) 26400       layer_361_block5f_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_363_block5f_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_362_block5f_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_364_block5f_activation (A (None, 16, 12, 1056) 0           layer_363_block5f_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_365_block5f_se_squeeze (G (None, 1056)         0           layer_364_block5f_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_366_block5f_se_reshape (R (None, 1, 1, 1056)   0           layer_365_block5f_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_367_block5f_se_reduce (Co (None, 1, 1, 44)     46508       layer_366_block5f_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_368_block5f_se_expand (Co (None, 1, 1, 1056)   47520       layer_367_block5f_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_369_block5f_se_excite (Mu (None, 16, 12, 1056) 0           layer_364_block5f_activation[0][0\n",
      "                                                                 layer_368_block5f_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_370_block5f_project_conv  (None, 16, 12, 176)  185856      layer_369_block5f_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_371_block5f_project_bn (B (None, 16, 12, 176)  704         layer_370_block5f_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_372_block5f_drop (FixedDr (None, 16, 12, 176)  0           layer_371_block5f_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_373_block5f_add (Add)     (None, 16, 12, 176)  0           layer_372_block5f_drop[0][0]     \n",
      "                                                                 layer_358_block5e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_374_block5g_expand_conv ( (None, 16, 12, 1056) 185856      layer_373_block5f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_375_block5g_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_374_block5g_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_376_block5g_expand_activa (None, 16, 12, 1056) 0           layer_375_block5g_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_377_block5g_dwconv (Depth (None, 16, 12, 1056) 26400       layer_376_block5g_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_378_block5g_bn (BatchNorm (None, 16, 12, 1056) 4224        layer_377_block5g_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_379_block5g_activation (A (None, 16, 12, 1056) 0           layer_378_block5g_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_380_block5g_se_squeeze (G (None, 1056)         0           layer_379_block5g_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_381_block5g_se_reshape (R (None, 1, 1, 1056)   0           layer_380_block5g_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_382_block5g_se_reduce (Co (None, 1, 1, 44)     46508       layer_381_block5g_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_383_block5g_se_expand (Co (None, 1, 1, 1056)   47520       layer_382_block5g_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_384_block5g_se_excite (Mu (None, 16, 12, 1056) 0           layer_379_block5g_activation[0][0\n",
      "                                                                 layer_383_block5g_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_385_block5g_project_conv  (None, 16, 12, 176)  185856      layer_384_block5g_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_386_block5g_project_bn (B (None, 16, 12, 176)  704         layer_385_block5g_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_387_block5g_drop (FixedDr (None, 16, 12, 176)  0           layer_386_block5g_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_388_block5g_add (Add)     (None, 16, 12, 176)  0           layer_387_block5g_drop[0][0]     \n",
      "                                                                 layer_373_block5f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_389_block6a_expand_conv ( (None, 16, 12, 1056) 185856      layer_388_block5g_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_390_block6a_expand_bn (Ba (None, 16, 12, 1056) 4224        layer_389_block6a_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_391_block6a_expand_activa (None, 16, 12, 1056) 0           layer_390_block6a_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_392_block6a_dwconv (Depth (None, 8, 6, 1056)   26400       layer_391_block6a_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_393_block6a_bn (BatchNorm (None, 8, 6, 1056)   4224        layer_392_block6a_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_394_block6a_activation (A (None, 8, 6, 1056)   0           layer_393_block6a_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_395_block6a_se_squeeze (G (None, 1056)         0           layer_394_block6a_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_396_block6a_se_reshape (R (None, 1, 1, 1056)   0           layer_395_block6a_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_397_block6a_se_reduce (Co (None, 1, 1, 44)     46508       layer_396_block6a_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_398_block6a_se_expand (Co (None, 1, 1, 1056)   47520       layer_397_block6a_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_399_block6a_se_excite (Mu (None, 8, 6, 1056)   0           layer_394_block6a_activation[0][0\n",
      "                                                                 layer_398_block6a_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_400_block6a_project_conv  (None, 8, 6, 304)    321024      layer_399_block6a_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_401_block6a_project_bn (B (None, 8, 6, 304)    1216        layer_400_block6a_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_402_block6b_expand_conv ( (None, 8, 6, 1824)   554496      layer_401_block6a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_403_block6b_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_402_block6b_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_404_block6b_expand_activa (None, 8, 6, 1824)   0           layer_403_block6b_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_405_block6b_dwconv (Depth (None, 8, 6, 1824)   45600       layer_404_block6b_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_406_block6b_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_405_block6b_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_407_block6b_activation (A (None, 8, 6, 1824)   0           layer_406_block6b_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_408_block6b_se_squeeze (G (None, 1824)         0           layer_407_block6b_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_409_block6b_se_reshape (R (None, 1, 1, 1824)   0           layer_408_block6b_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_410_block6b_se_reduce (Co (None, 1, 1, 76)     138700      layer_409_block6b_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_411_block6b_se_expand (Co (None, 1, 1, 1824)   140448      layer_410_block6b_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_412_block6b_se_excite (Mu (None, 8, 6, 1824)   0           layer_407_block6b_activation[0][0\n",
      "                                                                 layer_411_block6b_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_413_block6b_project_conv  (None, 8, 6, 304)    554496      layer_412_block6b_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_414_block6b_project_bn (B (None, 8, 6, 304)    1216        layer_413_block6b_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_415_block6b_drop (FixedDr (None, 8, 6, 304)    0           layer_414_block6b_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_416_block6b_add (Add)     (None, 8, 6, 304)    0           layer_415_block6b_drop[0][0]     \n",
      "                                                                 layer_401_block6a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_417_block6c_expand_conv ( (None, 8, 6, 1824)   554496      layer_416_block6b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_418_block6c_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_417_block6c_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_419_block6c_expand_activa (None, 8, 6, 1824)   0           layer_418_block6c_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_420_block6c_dwconv (Depth (None, 8, 6, 1824)   45600       layer_419_block6c_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_421_block6c_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_420_block6c_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_422_block6c_activation (A (None, 8, 6, 1824)   0           layer_421_block6c_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_423_block6c_se_squeeze (G (None, 1824)         0           layer_422_block6c_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_424_block6c_se_reshape (R (None, 1, 1, 1824)   0           layer_423_block6c_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_425_block6c_se_reduce (Co (None, 1, 1, 76)     138700      layer_424_block6c_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_426_block6c_se_expand (Co (None, 1, 1, 1824)   140448      layer_425_block6c_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_427_block6c_se_excite (Mu (None, 8, 6, 1824)   0           layer_422_block6c_activation[0][0\n",
      "                                                                 layer_426_block6c_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_428_block6c_project_conv  (None, 8, 6, 304)    554496      layer_427_block6c_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_429_block6c_project_bn (B (None, 8, 6, 304)    1216        layer_428_block6c_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_430_block6c_drop (FixedDr (None, 8, 6, 304)    0           layer_429_block6c_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_431_block6c_add (Add)     (None, 8, 6, 304)    0           layer_430_block6c_drop[0][0]     \n",
      "                                                                 layer_416_block6b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_432_block6d_expand_conv ( (None, 8, 6, 1824)   554496      layer_431_block6c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_433_block6d_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_432_block6d_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_434_block6d_expand_activa (None, 8, 6, 1824)   0           layer_433_block6d_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_435_block6d_dwconv (Depth (None, 8, 6, 1824)   45600       layer_434_block6d_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_436_block6d_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_435_block6d_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_437_block6d_activation (A (None, 8, 6, 1824)   0           layer_436_block6d_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_438_block6d_se_squeeze (G (None, 1824)         0           layer_437_block6d_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_439_block6d_se_reshape (R (None, 1, 1, 1824)   0           layer_438_block6d_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_440_block6d_se_reduce (Co (None, 1, 1, 76)     138700      layer_439_block6d_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_441_block6d_se_expand (Co (None, 1, 1, 1824)   140448      layer_440_block6d_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_442_block6d_se_excite (Mu (None, 8, 6, 1824)   0           layer_437_block6d_activation[0][0\n",
      "                                                                 layer_441_block6d_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_443_block6d_project_conv  (None, 8, 6, 304)    554496      layer_442_block6d_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_444_block6d_project_bn (B (None, 8, 6, 304)    1216        layer_443_block6d_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_445_block6d_drop (FixedDr (None, 8, 6, 304)    0           layer_444_block6d_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_446_block6d_add (Add)     (None, 8, 6, 304)    0           layer_445_block6d_drop[0][0]     \n",
      "                                                                 layer_431_block6c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_447_block6e_expand_conv ( (None, 8, 6, 1824)   554496      layer_446_block6d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_448_block6e_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_447_block6e_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_449_block6e_expand_activa (None, 8, 6, 1824)   0           layer_448_block6e_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_450_block6e_dwconv (Depth (None, 8, 6, 1824)   45600       layer_449_block6e_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_451_block6e_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_450_block6e_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_452_block6e_activation (A (None, 8, 6, 1824)   0           layer_451_block6e_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_453_block6e_se_squeeze (G (None, 1824)         0           layer_452_block6e_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_454_block6e_se_reshape (R (None, 1, 1, 1824)   0           layer_453_block6e_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_455_block6e_se_reduce (Co (None, 1, 1, 76)     138700      layer_454_block6e_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_456_block6e_se_expand (Co (None, 1, 1, 1824)   140448      layer_455_block6e_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_457_block6e_se_excite (Mu (None, 8, 6, 1824)   0           layer_452_block6e_activation[0][0\n",
      "                                                                 layer_456_block6e_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_458_block6e_project_conv  (None, 8, 6, 304)    554496      layer_457_block6e_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_459_block6e_project_bn (B (None, 8, 6, 304)    1216        layer_458_block6e_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_460_block6e_drop (FixedDr (None, 8, 6, 304)    0           layer_459_block6e_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_461_block6e_add (Add)     (None, 8, 6, 304)    0           layer_460_block6e_drop[0][0]     \n",
      "                                                                 layer_446_block6d_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_462_block6f_expand_conv ( (None, 8, 6, 1824)   554496      layer_461_block6e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_463_block6f_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_462_block6f_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_464_block6f_expand_activa (None, 8, 6, 1824)   0           layer_463_block6f_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_465_block6f_dwconv (Depth (None, 8, 6, 1824)   45600       layer_464_block6f_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_466_block6f_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_465_block6f_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_467_block6f_activation (A (None, 8, 6, 1824)   0           layer_466_block6f_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_468_block6f_se_squeeze (G (None, 1824)         0           layer_467_block6f_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_469_block6f_se_reshape (R (None, 1, 1, 1824)   0           layer_468_block6f_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_470_block6f_se_reduce (Co (None, 1, 1, 76)     138700      layer_469_block6f_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_471_block6f_se_expand (Co (None, 1, 1, 1824)   140448      layer_470_block6f_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_472_block6f_se_excite (Mu (None, 8, 6, 1824)   0           layer_467_block6f_activation[0][0\n",
      "                                                                 layer_471_block6f_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_473_block6f_project_conv  (None, 8, 6, 304)    554496      layer_472_block6f_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_474_block6f_project_bn (B (None, 8, 6, 304)    1216        layer_473_block6f_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_475_block6f_drop (FixedDr (None, 8, 6, 304)    0           layer_474_block6f_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_476_block6f_add (Add)     (None, 8, 6, 304)    0           layer_475_block6f_drop[0][0]     \n",
      "                                                                 layer_461_block6e_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_477_block6g_expand_conv ( (None, 8, 6, 1824)   554496      layer_476_block6f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_478_block6g_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_477_block6g_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_479_block6g_expand_activa (None, 8, 6, 1824)   0           layer_478_block6g_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_480_block6g_dwconv (Depth (None, 8, 6, 1824)   45600       layer_479_block6g_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_481_block6g_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_480_block6g_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_482_block6g_activation (A (None, 8, 6, 1824)   0           layer_481_block6g_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_483_block6g_se_squeeze (G (None, 1824)         0           layer_482_block6g_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_484_block6g_se_reshape (R (None, 1, 1, 1824)   0           layer_483_block6g_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_485_block6g_se_reduce (Co (None, 1, 1, 76)     138700      layer_484_block6g_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_486_block6g_se_expand (Co (None, 1, 1, 1824)   140448      layer_485_block6g_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_487_block6g_se_excite (Mu (None, 8, 6, 1824)   0           layer_482_block6g_activation[0][0\n",
      "                                                                 layer_486_block6g_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_488_block6g_project_conv  (None, 8, 6, 304)    554496      layer_487_block6g_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_489_block6g_project_bn (B (None, 8, 6, 304)    1216        layer_488_block6g_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_490_block6g_drop (FixedDr (None, 8, 6, 304)    0           layer_489_block6g_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_491_block6g_add (Add)     (None, 8, 6, 304)    0           layer_490_block6g_drop[0][0]     \n",
      "                                                                 layer_476_block6f_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_492_block6h_expand_conv ( (None, 8, 6, 1824)   554496      layer_491_block6g_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_493_block6h_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_492_block6h_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_494_block6h_expand_activa (None, 8, 6, 1824)   0           layer_493_block6h_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_495_block6h_dwconv (Depth (None, 8, 6, 1824)   45600       layer_494_block6h_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_496_block6h_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_495_block6h_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_497_block6h_activation (A (None, 8, 6, 1824)   0           layer_496_block6h_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_498_block6h_se_squeeze (G (None, 1824)         0           layer_497_block6h_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_499_block6h_se_reshape (R (None, 1, 1, 1824)   0           layer_498_block6h_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_500_block6h_se_reduce (Co (None, 1, 1, 76)     138700      layer_499_block6h_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_501_block6h_se_expand (Co (None, 1, 1, 1824)   140448      layer_500_block6h_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_502_block6h_se_excite (Mu (None, 8, 6, 1824)   0           layer_497_block6h_activation[0][0\n",
      "                                                                 layer_501_block6h_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_503_block6h_project_conv  (None, 8, 6, 304)    554496      layer_502_block6h_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_504_block6h_project_bn (B (None, 8, 6, 304)    1216        layer_503_block6h_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_505_block6h_drop (FixedDr (None, 8, 6, 304)    0           layer_504_block6h_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_506_block6h_add (Add)     (None, 8, 6, 304)    0           layer_505_block6h_drop[0][0]     \n",
      "                                                                 layer_491_block6g_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_507_block6i_expand_conv ( (None, 8, 6, 1824)   554496      layer_506_block6h_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_508_block6i_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_507_block6i_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_509_block6i_expand_activa (None, 8, 6, 1824)   0           layer_508_block6i_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_510_block6i_dwconv (Depth (None, 8, 6, 1824)   45600       layer_509_block6i_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_511_block6i_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_510_block6i_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_512_block6i_activation (A (None, 8, 6, 1824)   0           layer_511_block6i_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_513_block6i_se_squeeze (G (None, 1824)         0           layer_512_block6i_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_514_block6i_se_reshape (R (None, 1, 1, 1824)   0           layer_513_block6i_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_515_block6i_se_reduce (Co (None, 1, 1, 76)     138700      layer_514_block6i_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_516_block6i_se_expand (Co (None, 1, 1, 1824)   140448      layer_515_block6i_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_517_block6i_se_excite (Mu (None, 8, 6, 1824)   0           layer_512_block6i_activation[0][0\n",
      "                                                                 layer_516_block6i_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_518_block6i_project_conv  (None, 8, 6, 304)    554496      layer_517_block6i_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_519_block6i_project_bn (B (None, 8, 6, 304)    1216        layer_518_block6i_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_520_block6i_drop (FixedDr (None, 8, 6, 304)    0           layer_519_block6i_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_521_block6i_add (Add)     (None, 8, 6, 304)    0           layer_520_block6i_drop[0][0]     \n",
      "                                                                 layer_506_block6h_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_522_block7a_expand_conv ( (None, 8, 6, 1824)   554496      layer_521_block6i_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_523_block7a_expand_bn (Ba (None, 8, 6, 1824)   7296        layer_522_block7a_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_524_block7a_expand_activa (None, 8, 6, 1824)   0           layer_523_block7a_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_525_block7a_dwconv (Depth (None, 8, 6, 1824)   16416       layer_524_block7a_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_526_block7a_bn (BatchNorm (None, 8, 6, 1824)   7296        layer_525_block7a_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_527_block7a_activation (A (None, 8, 6, 1824)   0           layer_526_block7a_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_528_block7a_se_squeeze (G (None, 1824)         0           layer_527_block7a_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_529_block7a_se_reshape (R (None, 1, 1, 1824)   0           layer_528_block7a_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_530_block7a_se_reduce (Co (None, 1, 1, 76)     138700      layer_529_block7a_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_531_block7a_se_expand (Co (None, 1, 1, 1824)   140448      layer_530_block7a_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_532_block7a_se_excite (Mu (None, 8, 6, 1824)   0           layer_527_block7a_activation[0][0\n",
      "                                                                 layer_531_block7a_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_533_block7a_project_conv  (None, 8, 6, 512)    933888      layer_532_block7a_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_534_block7a_project_bn (B (None, 8, 6, 512)    2048        layer_533_block7a_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_535_block7b_expand_conv ( (None, 8, 6, 3072)   1572864     layer_534_block7a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_536_block7b_expand_bn (Ba (None, 8, 6, 3072)   12288       layer_535_block7b_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_537_block7b_expand_activa (None, 8, 6, 3072)   0           layer_536_block7b_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_538_block7b_dwconv (Depth (None, 8, 6, 3072)   27648       layer_537_block7b_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_539_block7b_bn (BatchNorm (None, 8, 6, 3072)   12288       layer_538_block7b_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_540_block7b_activation (A (None, 8, 6, 3072)   0           layer_539_block7b_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_541_block7b_se_squeeze (G (None, 3072)         0           layer_540_block7b_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_542_block7b_se_reshape (R (None, 1, 1, 3072)   0           layer_541_block7b_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_543_block7b_se_reduce (Co (None, 1, 1, 128)    393344      layer_542_block7b_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_544_block7b_se_expand (Co (None, 1, 1, 3072)   396288      layer_543_block7b_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_545_block7b_se_excite (Mu (None, 8, 6, 3072)   0           layer_540_block7b_activation[0][0\n",
      "                                                                 layer_544_block7b_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_546_block7b_project_conv  (None, 8, 6, 512)    1572864     layer_545_block7b_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_547_block7b_project_bn (B (None, 8, 6, 512)    2048        layer_546_block7b_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_548_block7b_drop (FixedDr (None, 8, 6, 512)    0           layer_547_block7b_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_549_block7b_add (Add)     (None, 8, 6, 512)    0           layer_548_block7b_drop[0][0]     \n",
      "                                                                 layer_534_block7a_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_550_block7c_expand_conv ( (None, 8, 6, 3072)   1572864     layer_549_block7b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_551_block7c_expand_bn (Ba (None, 8, 6, 3072)   12288       layer_550_block7c_expand_conv[0][\n",
      "__________________________________________________________________________________________________\n",
      "layer_552_block7c_expand_activa (None, 8, 6, 3072)   0           layer_551_block7c_expand_bn[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_553_block7c_dwconv (Depth (None, 8, 6, 3072)   27648       layer_552_block7c_expand_activati\n",
      "__________________________________________________________________________________________________\n",
      "layer_554_block7c_bn (BatchNorm (None, 8, 6, 3072)   12288       layer_553_block7c_dwconv[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "layer_555_block7c_activation (A (None, 8, 6, 3072)   0           layer_554_block7c_bn[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_556_block7c_se_squeeze (G (None, 3072)         0           layer_555_block7c_activation[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_557_block7c_se_reshape (R (None, 1, 1, 3072)   0           layer_556_block7c_se_squeeze[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_558_block7c_se_reduce (Co (None, 1, 1, 128)    393344      layer_557_block7c_se_reshape[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_559_block7c_se_expand (Co (None, 1, 1, 3072)   396288      layer_558_block7c_se_reduce[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_560_block7c_se_excite (Mu (None, 8, 6, 3072)   0           layer_555_block7c_activation[0][0\n",
      "                                                                 layer_559_block7c_se_expand[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_561_block7c_project_conv  (None, 8, 6, 512)    1572864     layer_560_block7c_se_excite[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_562_block7c_project_bn (B (None, 8, 6, 512)    2048        layer_561_block7c_project_conv[0]\n",
      "__________________________________________________________________________________________________\n",
      "layer_563_block7c_drop (FixedDr (None, 8, 6, 512)    0           layer_562_block7c_project_bn[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_564_block7c_add (Add)     (None, 8, 6, 512)    0           layer_563_block7c_drop[0][0]     \n",
      "                                                                 layer_549_block7b_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_565_top_conv (Conv2D)     (None, 8, 6, 2048)   1048576     layer_564_block7c_add[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_566_top_bn (BatchNormaliz (None, 8, 6, 2048)   8192        layer_565_top_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "layer_567_top_activation (Activ (None, 8, 6, 2048)   0           layer_566_top_bn[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 2048)         0           layer_567_top_activation[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1024)         2098176     global_average_pooling2d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1024)         1049600     dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 1024)         1049600     dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 256)          262400      dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 256)          65792       dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 256)          65792       dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 256)          65792       dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lat (Dense)                     (None, 1)            1025        dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "long (Dense)                    (None, 1)            257         dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "depth (Dense)                   (None, 1)            257         dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "magnitude (Dense)               (None, 1)            257         dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time (Dense)                    (None, 1)            257         dense_49[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,172,725\n",
      "Trainable params: 32,999,989\n",
      "Non-trainable params: 172,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_reg['multi_lat_long_depth_magnitude_time'].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing proses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah folder test: 1\n"
     ]
    }
   ],
   "source": [
    "folder_list = list_folder_for_test\n",
    "print('Jumlah folder test:',len(folder_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prediction_final(ncheck, folder_list, output_reg):\n",
    "#     %%time\n",
    "    pred_images_start = [] \n",
    "    pred_images_end = [] \n",
    "    pred_reg = {}\n",
    "    for out_reg in output_reg:\n",
    "        if out_reg.startswith('m'):\n",
    "            output_names = [\"multi_\"+__ for __ in out_reg.split('_')[1:]]\n",
    "            for col in output_names:\n",
    "                pred_reg[col] = []\n",
    "        else:\n",
    "            pred_reg[out_reg] = []\n",
    "\n",
    "\n",
    "    ncheck = ncheck\n",
    "\n",
    "    for _im,folder in enumerate(folder_list):\n",
    "        directory = dataset_path+folder+'/'\n",
    "        predicted_label = []\n",
    "        print(_im,'Predicting',folder,'with',len([img for img in os.listdir(directory) if len(img)>2]),'images', end=' ')\n",
    "        flag = np.zeros([ncheck])\n",
    "        i = 0\n",
    "        nothing = True\n",
    "        for filename in [img for img in os.listdir(directory) if len(img)>2]:\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                start_time = time.time()\n",
    "                img = image.load_img(directory+filename, target_size=(x.shape[0], x.shape[1]))\n",
    "                img = image.img_to_array(img)\n",
    "                img = np.expand_dims(img, axis=0)/255\n",
    "\n",
    "                images = np.vstack([img])\n",
    "                classes = model_clf.predict(images)\n",
    "                predicted_label.append(classes[0][0])\n",
    "\n",
    "                if int(filename[:6])%1000==0:\n",
    "                    print('.', end=' ')\n",
    "\n",
    "                if float(classes[0][0]) > 0.5: \n",
    "                    flag[i] = 1\n",
    "                    i += 1\n",
    "                    if sum(flag) == ncheck:\n",
    "                        print('')\n",
    "                        pred_images_start.append(str(filename.split('.')[0])) # modif\n",
    "                        pred_images_end.append(str(filename.split('.')[0]))\n",
    "                        print('-->',str(filename.split('.')[0]),'({})'.format(filename.split('.')[0]))\n",
    "                        for out_reg in output_reg:\n",
    "                            if out_reg.startswith('m'):\n",
    "                                pred_multi = model_reg[out_reg].predict(images) ## kode ini untuk model multi regresi\n",
    "#                                 print(model_reg[out_reg].name, pred_multi) ####### DEBUG\n",
    "                                for i_,col in enumerate(output_names):\n",
    "                                    pred_reg[col].append(float(pred_multi[i_].squeeze()))\n",
    "                                    print(\"pred_\"+col,pred_multi[i_].squeeze(), '-', col.split(\"_\")[-1], df[df.foldername==folder]['norm_'+col.split(\"_\")[-1]].values,)\n",
    "                            else:\n",
    "                                pred_single = model_reg[out_reg].predict(images) ## kode ini untuk model single regresi\n",
    "#                                 print(model_reg[out_reg].name, pred_single) ####### DEBUG\n",
    "                                pred_reg[out_reg].append(float(pred_single.squeeze()))\n",
    "                                print(\"pred_\"+out_reg,pred_single, '-', out_reg.split(\"_\")[-1], df[df.foldername==folder]['norm_'+out_reg.split(\"_\")[-1]].values,)\n",
    "                                \n",
    "#                         nothing = False\n",
    "#                         print()\n",
    "#                         break\n",
    "                        i = 0\n",
    "                else:\n",
    "                    flag = np.zeros([ncheck])\n",
    "                    i = 0\n",
    "        if nothing:\n",
    "            pred_images_start.append(np.nan)\n",
    "            pred_images_end.append(np.nan)  \n",
    "            for out_reg in output_reg:\n",
    "                if out_reg.startswith('m'):\n",
    "                    for col in output_names:\n",
    "                        pred_reg[col].append(np.nan)\n",
    "                else:\n",
    "                    pred_reg[out_reg].append(np.nan)\n",
    "                \n",
    "    return pred_images_start,pred_images_end,pred_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08 11:20:26.350852\n",
      "0 Predicting 20170428_014929crop with 801 images "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". \n",
      "--> 000056_20170428_014929_crop (000056_20170428_014929_crop)\n",
      "pred_multi_lat -0.05341686 - lat [nan]\n",
      "pred_multi_long -0.30017427 - long [nan]\n",
      "pred_multi_depth 0.1656674 - depth [nan]\n",
      "pred_multi_magnitude 0.108100265 - magnitude [nan]\n",
      "pred_multi_time 0.024671335 - time [nan]\n",
      "\n",
      "--> 000060_20170428_014929_crop (000060_20170428_014929_crop)\n",
      "pred_multi_lat -0.04567828 - lat [nan]\n",
      "pred_multi_long -0.29216012 - long [nan]\n",
      "pred_multi_depth 0.15973687 - depth [nan]\n",
      "pred_multi_magnitude 0.10488318 - magnitude [nan]\n",
      "pred_multi_time 0.021503642 - time [nan]\n",
      "\n",
      "--> 000061_20170428_014929_crop (000061_20170428_014929_crop)\n",
      "pred_multi_lat -0.046091177 - lat [nan]\n",
      "pred_multi_long -0.2836802 - long [nan]\n",
      "pred_multi_depth 0.15242998 - depth [nan]\n",
      "pred_multi_magnitude 0.10220262 - magnitude [nan]\n",
      "pred_multi_time 0.025392905 - time [nan]\n",
      "\n",
      "--> 000062_20170428_014929_crop (000062_20170428_014929_crop)\n",
      "pred_multi_lat -0.044565298 - lat [nan]\n",
      "pred_multi_long -0.2929237 - long [nan]\n",
      "pred_multi_depth 0.17227392 - depth [nan]\n",
      "pred_multi_magnitude 0.10588589 - magnitude [nan]\n",
      "pred_multi_time 0.030122902 - time [nan]\n",
      "\n",
      "--> 000065_20170428_014929_crop (000065_20170428_014929_crop)\n",
      "pred_multi_lat -0.0374773 - lat [nan]\n",
      "pred_multi_long -0.2932323 - long [nan]\n",
      "pred_multi_depth 0.15914035 - depth [nan]\n",
      "pred_multi_magnitude 0.10181668 - magnitude [nan]\n",
      "pred_multi_time 0.026328063 - time [nan]\n",
      "\n",
      "--> 000066_20170428_014929_crop (000066_20170428_014929_crop)\n",
      "pred_multi_lat -0.034758113 - lat [nan]\n",
      "pred_multi_long -0.3048041 - long [nan]\n",
      "pred_multi_depth 0.17044978 - depth [nan]\n",
      "pred_multi_magnitude 0.10312211 - magnitude [nan]\n",
      "pred_multi_time 0.031668738 - time [nan]\n",
      "\n",
      "--> 000081_20170428_014929_crop (000081_20170428_014929_crop)\n",
      "pred_multi_lat -0.05726978 - lat [nan]\n",
      "pred_multi_long -0.31068525 - long [nan]\n",
      "pred_multi_depth 0.16000849 - depth [nan]\n",
      "pred_multi_magnitude 0.11733531 - magnitude [nan]\n",
      "pred_multi_time 0.018578382 - time [nan]\n",
      "\n",
      "--> 000296_20170428_014929_crop (000296_20170428_014929_crop)\n",
      "pred_multi_lat -0.017630989 - lat [nan]\n",
      "pred_multi_long -0.25331408 - long [nan]\n",
      "pred_multi_depth 0.12286881 - depth [nan]\n",
      "pred_multi_magnitude 0.12058605 - magnitude [nan]\n",
      "pred_multi_time 0.030639675 - time [nan]\n",
      "\n",
      "--> 000298_20170428_014929_crop (000298_20170428_014929_crop)\n",
      "pred_multi_lat -0.0025241636 - lat [nan]\n",
      "pred_multi_long -0.2676857 - long [nan]\n",
      "pred_multi_depth 0.12596929 - depth [nan]\n",
      "pred_multi_magnitude 0.11683454 - magnitude [nan]\n",
      "pred_multi_time 0.027621742 - time [nan]\n",
      "\n",
      "--> 000299_20170428_014929_crop (000299_20170428_014929_crop)\n",
      "pred_multi_lat -0.032787144 - lat [nan]\n",
      "pred_multi_long -0.21803641 - long [nan]\n",
      "pred_multi_depth 0.07546398 - depth [nan]\n",
      "pred_multi_magnitude 0.12360693 - magnitude [nan]\n",
      "pred_multi_time 0.023625478 - time [nan]\n",
      "\n",
      "--> 000300_20170428_014929_crop (000300_20170428_014929_crop)\n",
      "pred_multi_lat -0.031211263 - lat [nan]\n",
      "pred_multi_long -0.21043444 - long [nan]\n",
      "pred_multi_depth 0.06942679 - depth [nan]\n",
      "pred_multi_magnitude 0.117187776 - magnitude [nan]\n",
      "pred_multi_time 0.03173406 - time [nan]\n",
      "\n",
      "--> 000301_20170428_014929_crop (000301_20170428_014929_crop)\n",
      "pred_multi_lat -0.016997907 - lat [nan]\n",
      "pred_multi_long -0.21320601 - long [nan]\n",
      "pred_multi_depth 0.07739577 - depth [nan]\n",
      "pred_multi_magnitude 0.117008395 - magnitude [nan]\n",
      "pred_multi_time 0.0274905 - time [nan]\n",
      "\n",
      "--> 000302_20170428_014929_crop (000302_20170428_014929_crop)\n",
      "pred_multi_lat -0.02552201 - lat [nan]\n",
      "pred_multi_long -0.20583493 - long [nan]\n",
      "pred_multi_depth 0.08435336 - depth [nan]\n",
      "pred_multi_magnitude 0.11285541 - magnitude [nan]\n",
      "pred_multi_time 0.030361084 - time [nan]\n",
      "\n",
      "--> 000303_20170428_014929_crop (000303_20170428_014929_crop)\n",
      "pred_multi_lat -0.025399135 - lat [nan]\n",
      "pred_multi_long -0.2601527 - long [nan]\n",
      "pred_multi_depth 0.13591978 - depth [nan]\n",
      "pred_multi_magnitude 0.11926 - magnitude [nan]\n",
      "pred_multi_time 0.027395371 - time [nan]\n",
      "\n",
      "--> 000304_20170428_014929_crop (000304_20170428_014929_crop)\n",
      "pred_multi_lat -0.02195396 - lat [nan]\n",
      "pred_multi_long -0.25925112 - long [nan]\n",
      "pred_multi_depth 0.11955865 - depth [nan]\n",
      "pred_multi_magnitude 0.12637088 - magnitude [nan]\n",
      "pred_multi_time 0.028343782 - time [nan]\n",
      "\n",
      "--> 000305_20170428_014929_crop (000305_20170428_014929_crop)\n",
      "pred_multi_lat -0.028318143 - lat [nan]\n",
      "pred_multi_long -0.19685341 - long [nan]\n",
      "pred_multi_depth 0.06809278 - depth [nan]\n",
      "pred_multi_magnitude 0.103640996 - magnitude [nan]\n",
      "pred_multi_time 0.029876266 - time [nan]\n",
      "\n",
      "--> 000306_20170428_014929_crop (000306_20170428_014929_crop)\n",
      "pred_multi_lat -0.025541663 - lat [nan]\n",
      "pred_multi_long -0.19511057 - long [nan]\n",
      "pred_multi_depth 0.07541538 - depth [nan]\n",
      "pred_multi_magnitude 0.119441584 - magnitude [nan]\n",
      "pred_multi_time 0.03683338 - time [nan]\n",
      "\n",
      "--> 000307_20170428_014929_crop (000307_20170428_014929_crop)\n",
      "pred_multi_lat -0.01630665 - lat [nan]\n",
      "pred_multi_long -0.24706411 - long [nan]\n",
      "pred_multi_depth 0.119452946 - depth [nan]\n",
      "pred_multi_magnitude 0.119807795 - magnitude [nan]\n",
      "pred_multi_time 0.027161572 - time [nan]\n",
      "\n",
      "--> 000308_20170428_014929_crop (000308_20170428_014929_crop)\n",
      "pred_multi_lat -0.02255435 - lat [nan]\n",
      "pred_multi_long -0.21443793 - long [nan]\n",
      "pred_multi_depth 0.103610724 - depth [nan]\n",
      "pred_multi_magnitude 0.12736058 - magnitude [nan]\n",
      "pred_multi_time 0.023211725 - time [nan]\n",
      "\n",
      "--> 000309_20170428_014929_crop (000309_20170428_014929_crop)\n",
      "pred_multi_lat -0.021022204 - lat [nan]\n",
      "pred_multi_long -0.20351247 - long [nan]\n",
      "pred_multi_depth 0.071480215 - depth [nan]\n",
      "pred_multi_magnitude 0.10691544 - magnitude [nan]\n",
      "pred_multi_time 0.028420119 - time [nan]\n",
      "\n",
      "--> 000310_20170428_014929_crop (000310_20170428_014929_crop)\n",
      "pred_multi_lat -0.020527927 - lat [nan]\n",
      "pred_multi_long -0.22895813 - long [nan]\n",
      "pred_multi_depth 0.0953155 - depth [nan]\n",
      "pred_multi_magnitude 0.12541264 - magnitude [nan]\n",
      "pred_multi_time 0.028417926 - time [nan]\n",
      "\n",
      "--> 000311_20170428_014929_crop (000311_20170428_014929_crop)\n",
      "pred_multi_lat -0.013764054 - lat [nan]\n",
      "pred_multi_long -0.249297 - long [nan]\n",
      "pred_multi_depth 0.115166485 - depth [nan]\n",
      "pred_multi_magnitude 0.11909898 - magnitude [nan]\n",
      "pred_multi_time 0.028306335 - time [nan]\n",
      "\n",
      "--> 000312_20170428_014929_crop (000312_20170428_014929_crop)\n",
      "pred_multi_lat -0.01974662 - lat [nan]\n",
      "pred_multi_long -0.25702867 - long [nan]\n",
      "pred_multi_depth 0.11436789 - depth [nan]\n",
      "pred_multi_magnitude 0.1285663 - magnitude [nan]\n",
      "pred_multi_time 0.029075062 - time [nan]\n",
      "\n",
      "--> 000313_20170428_014929_crop (000313_20170428_014929_crop)\n",
      "pred_multi_lat -0.01370535 - lat [nan]\n",
      "pred_multi_long -0.26276204 - long [nan]\n",
      "pred_multi_depth 0.13869034 - depth [nan]\n",
      "pred_multi_magnitude 0.12057424 - magnitude [nan]\n",
      "pred_multi_time 0.030094877 - time [nan]\n",
      "\n",
      "--> 000314_20170428_014929_crop (000314_20170428_014929_crop)\n",
      "pred_multi_lat -0.01316588 - lat [nan]\n",
      "pred_multi_long -0.18834163 - long [nan]\n",
      "pred_multi_depth 0.08122315 - depth [nan]\n",
      "pred_multi_magnitude 0.11389152 - magnitude [nan]\n",
      "pred_multi_time 0.026336996 - time [nan]\n",
      "\n",
      "--> 000315_20170428_014929_crop (000315_20170428_014929_crop)\n",
      "pred_multi_lat -0.041143835 - lat [nan]\n",
      "pred_multi_long -0.20775072 - long [nan]\n",
      "pred_multi_depth 0.08486889 - depth [nan]\n",
      "pred_multi_magnitude 0.11935002 - magnitude [nan]\n",
      "pred_multi_time 0.024074107 - time [nan]\n",
      "\n",
      "--> 000316_20170428_014929_crop (000316_20170428_014929_crop)\n",
      "pred_multi_lat -0.023114942 - lat [nan]\n",
      "pred_multi_long -0.21287079 - long [nan]\n",
      "pred_multi_depth 0.09099831 - depth [nan]\n",
      "pred_multi_magnitude 0.12872116 - magnitude [nan]\n",
      "pred_multi_time 0.025035663 - time [nan]\n",
      "\n",
      "--> 000317_20170428_014929_crop (000317_20170428_014929_crop)\n",
      "pred_multi_lat -0.022185024 - lat [nan]\n",
      "pred_multi_long -0.21421114 - long [nan]\n",
      "pred_multi_depth 0.08002992 - depth [nan]\n",
      "pred_multi_magnitude 0.121104375 - magnitude [nan]\n",
      "pred_multi_time 0.03429453 - time [nan]\n",
      "\n",
      "--> 000318_20170428_014929_crop (000318_20170428_014929_crop)\n",
      "pred_multi_lat -0.014164202 - lat [nan]\n",
      "pred_multi_long -0.19521332 - long [nan]\n",
      "pred_multi_depth 0.08113814 - depth [nan]\n",
      "pred_multi_magnitude 0.105874635 - magnitude [nan]\n",
      "pred_multi_time 0.032440785 - time [nan]\n",
      "\n",
      "--> 000319_20170428_014929_crop (000319_20170428_014929_crop)\n",
      "pred_multi_lat -0.01724008 - lat [nan]\n",
      "pred_multi_long -0.20970544 - long [nan]\n",
      "pred_multi_depth 0.09368381 - depth [nan]\n",
      "pred_multi_magnitude 0.11008396 - magnitude [nan]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_multi_time 0.024006575 - time [nan]\n",
      "\n",
      "--> 000320_20170428_014929_crop (000320_20170428_014929_crop)\n",
      "pred_multi_lat -0.021702185 - lat [nan]\n",
      "pred_multi_long -0.1662208 - long [nan]\n",
      "pred_multi_depth 0.07164024 - depth [nan]\n",
      "pred_multi_magnitude 0.115829825 - magnitude [nan]\n",
      "pred_multi_time 0.027235072 - time [nan]\n",
      "\n",
      "--> 000321_20170428_014929_crop (000321_20170428_014929_crop)\n",
      "pred_multi_lat -0.004348684 - lat [nan]\n",
      "pred_multi_long -0.22723818 - long [nan]\n",
      "pred_multi_depth 0.08922904 - depth [nan]\n",
      "pred_multi_magnitude 0.13347208 - magnitude [nan]\n",
      "pred_multi_time 0.029371563 - time [nan]\n",
      "\n",
      "--> 000322_20170428_014929_crop (000322_20170428_014929_crop)\n",
      "pred_multi_lat -0.014532378 - lat [nan]\n",
      "pred_multi_long -0.19473484 - long [nan]\n",
      "pred_multi_depth 0.0792654 - depth [nan]\n",
      "pred_multi_magnitude 0.10975332 - magnitude [nan]\n",
      "pred_multi_time 0.034130372 - time [nan]\n",
      "\n",
      "--> 000323_20170428_014929_crop (000323_20170428_014929_crop)\n",
      "pred_multi_lat -0.01570363 - lat [nan]\n",
      "pred_multi_long -0.23573524 - long [nan]\n",
      "pred_multi_depth 0.10001287 - depth [nan]\n",
      "pred_multi_magnitude 0.1310816 - magnitude [nan]\n",
      "pred_multi_time 0.030240297 - time [nan]\n",
      "\n",
      "--> 000324_20170428_014929_crop (000324_20170428_014929_crop)\n",
      "pred_multi_lat -0.024742903 - lat [nan]\n",
      "pred_multi_long -0.20508838 - long [nan]\n",
      "pred_multi_depth 0.086943924 - depth [nan]\n",
      "pred_multi_magnitude 0.11772478 - magnitude [nan]\n",
      "pred_multi_time 0.023029895 - time [nan]\n",
      "\n",
      "--> 000325_20170428_014929_crop (000325_20170428_014929_crop)\n",
      "pred_multi_lat -0.0143861715 - lat [nan]\n",
      "pred_multi_long -0.22707547 - long [nan]\n",
      "pred_multi_depth 0.07772056 - depth [nan]\n",
      "pred_multi_magnitude 0.13232818 - magnitude [nan]\n",
      "pred_multi_time 0.036236286 - time [nan]\n",
      "\n",
      "--> 000326_20170428_014929_crop (000326_20170428_014929_crop)\n",
      "pred_multi_lat -0.0135248415 - lat [nan]\n",
      "pred_multi_long -0.2126985 - long [nan]\n",
      "pred_multi_depth 0.08452928 - depth [nan]\n",
      "pred_multi_magnitude 0.12112417 - magnitude [nan]\n",
      "pred_multi_time 0.030577593 - time [nan]\n",
      "\n",
      "--> 000327_20170428_014929_crop (000327_20170428_014929_crop)\n",
      "pred_multi_lat -0.010795601 - lat [nan]\n",
      "pred_multi_long -0.24343193 - long [nan]\n",
      "pred_multi_depth 0.08671609 - depth [nan]\n",
      "pred_multi_magnitude 0.12426299 - magnitude [nan]\n",
      "pred_multi_time 0.038924627 - time [nan]\n",
      "\n",
      "--> 000328_20170428_014929_crop (000328_20170428_014929_crop)\n",
      "pred_multi_lat -0.020810299 - lat [nan]\n",
      "pred_multi_long -0.29009664 - long [nan]\n",
      "pred_multi_depth 0.14435902 - depth [nan]\n",
      "pred_multi_magnitude 0.12857015 - magnitude [nan]\n",
      "pred_multi_time 0.03466875 - time [nan]\n",
      "\n",
      "--> 000329_20170428_014929_crop (000329_20170428_014929_crop)\n",
      "pred_multi_lat -0.014871024 - lat [nan]\n",
      "pred_multi_long -0.28509563 - long [nan]\n",
      "pred_multi_depth 0.141084 - depth [nan]\n",
      "pred_multi_magnitude 0.13709933 - magnitude [nan]\n",
      "pred_multi_time 0.027864195 - time [nan]\n",
      "2021-04-08 11:21:04.183251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37.831825400000525"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now())\n",
    "st_time = time.clock()\n",
    "\n",
    "folder_save_name = \"test_EFnetB5_E15_26_N1_P-best\" # Nama Folder Save\n",
    "\n",
    "if not os.path.exists(model_save_path+folder_save_name):\n",
    "    os.mkdir(model_save_path+folder_save_name)\n",
    "ncheck = 1\n",
    "model_list = ['multi_lat_long_depth_magnitude_time']\n",
    "pred_images_start,pred_images_end,pred_reg = prediction_final(ncheck,folder_list, model_list) # Fungsi Prediction Final\n",
    "\n",
    "print(datetime.now())\n",
    "time.clock() - st_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 41 41 "
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(folder_list),end=' ')\n",
    "print(len(pred_images_start),end=' ')\n",
    "print(len(pred_images_end),end=' ')\n",
    "# print(len(pred_reg))\n",
    "\n",
    "# create csv\n",
    "df_test = pd.DataFrame({#'filename':[folder.replace('crop','') for folder in folder_list],\n",
    "              'flag_start':pred_images_start,\n",
    "              'flag_end':pred_images_end})\n",
    "\n",
    "# df_test['PA_start'] = [df[df.foldername==folder][\"PA_start_real\"].values for folder in folder_list]\n",
    "# df_test['PA_end'] = [df[df.foldername==folder][\"PA_start_real\"].values + 200 for folder in folder_list]\n",
    "\n",
    "for col in pred_reg.keys():\n",
    "    df_test['pred_'+col] = pred_reg[col]\n",
    "#     gt_ = []\n",
    "#     for folder in folder_list:\n",
    "#         gt_.append(float(df[df.foldername==folder]['norm_'+col.split(\"_\")[-1]].values))\n",
    "#     df_test['norm_'+col.split(\"_\")[-1]] = gt_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denormalization (Dont run 2x, because it will happen recursively replacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hanya untuk data normalisasi\n",
    "max_,min_ = {},{}\n",
    "max_['lat'] = -6.64264\n",
    "min_['lat'] = -11.5152\n",
    "max_['long'] = 115.033\n",
    "min_['long'] = 111.532\n",
    "max_['depth'] = 588.426\n",
    "min_['depth'] = 1.16\n",
    "max_['magnitude'] = 6.5\n",
    "min_['magnitude'] = 3.0\n",
    "max_['time'] = 74.122\n",
    "min_['time'] = 4.502\n",
    "\n",
    "# Denorm\n",
    "for col in pred_reg.keys():\n",
    "    c_ = col.split(\"_\")[-1]\n",
    "    df_test['pred_'+col] = df_test['pred_'+col].apply(lambda x: (x*(max_[c_] - min_[c_]))+min_[c_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flag_start</th>\n",
       "      <th>flag_end</th>\n",
       "      <th>pred_multi_lat</th>\n",
       "      <th>pred_multi_long</th>\n",
       "      <th>pred_multi_depth</th>\n",
       "      <th>pred_multi_magnitude</th>\n",
       "      <th>pred_multi_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000056_20170428_014929_crop</td>\n",
       "      <td>000056_20170428_014929_crop</td>\n",
       "      <td>-11.775477</td>\n",
       "      <td>110.481090</td>\n",
       "      <td>98.450831</td>\n",
       "      <td>3.378351</td>\n",
       "      <td>6.219618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000060_20170428_014929_crop</td>\n",
       "      <td>000060_20170428_014929_crop</td>\n",
       "      <td>-11.737770</td>\n",
       "      <td>110.509147</td>\n",
       "      <td>94.968034</td>\n",
       "      <td>3.367091</td>\n",
       "      <td>5.999084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000061_20170428_014929_crop</td>\n",
       "      <td>000061_20170428_014929_crop</td>\n",
       "      <td>-11.739782</td>\n",
       "      <td>110.538836</td>\n",
       "      <td>90.676946</td>\n",
       "      <td>3.357709</td>\n",
       "      <td>6.269854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000062_20170428_014929_crop</td>\n",
       "      <td>000062_20170428_014929_crop</td>\n",
       "      <td>-11.732347</td>\n",
       "      <td>110.506474</td>\n",
       "      <td>102.330615</td>\n",
       "      <td>3.370601</td>\n",
       "      <td>6.599156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000065_20170428_014929_crop</td>\n",
       "      <td>000065_20170428_014929_crop</td>\n",
       "      <td>-11.697810</td>\n",
       "      <td>110.505394</td>\n",
       "      <td>94.617716</td>\n",
       "      <td>3.356358</td>\n",
       "      <td>6.334960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000066_20170428_014929_crop</td>\n",
       "      <td>000066_20170428_014929_crop</td>\n",
       "      <td>-11.684561</td>\n",
       "      <td>110.464881</td>\n",
       "      <td>101.259360</td>\n",
       "      <td>3.360927</td>\n",
       "      <td>6.706778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000081_20170428_014929_crop</td>\n",
       "      <td>000081_20170428_014929_crop</td>\n",
       "      <td>-11.794250</td>\n",
       "      <td>110.444291</td>\n",
       "      <td>95.127546</td>\n",
       "      <td>3.410674</td>\n",
       "      <td>5.795427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000296_20170428_014929_crop</td>\n",
       "      <td>000296_20170428_014929_crop</td>\n",
       "      <td>-11.601108</td>\n",
       "      <td>110.645147</td>\n",
       "      <td>73.316677</td>\n",
       "      <td>3.422051</td>\n",
       "      <td>6.635134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000298_20170428_014929_crop</td>\n",
       "      <td>000298_20170428_014929_crop</td>\n",
       "      <td>-11.527499</td>\n",
       "      <td>110.594832</td>\n",
       "      <td>75.137481</td>\n",
       "      <td>3.408921</td>\n",
       "      <td>6.425026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000299_20170428_014929_crop</td>\n",
       "      <td>000299_20170428_014929_crop</td>\n",
       "      <td>-11.674957</td>\n",
       "      <td>110.768655</td>\n",
       "      <td>45.477430</td>\n",
       "      <td>3.432624</td>\n",
       "      <td>6.146806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>000300_20170428_014929_crop</td>\n",
       "      <td>000300_20170428_014929_crop</td>\n",
       "      <td>-11.667279</td>\n",
       "      <td>110.795269</td>\n",
       "      <td>41.931993</td>\n",
       "      <td>3.410157</td>\n",
       "      <td>6.711325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000301_20170428_014929_crop</td>\n",
       "      <td>000301_20170428_014929_crop</td>\n",
       "      <td>-11.598023</td>\n",
       "      <td>110.785566</td>\n",
       "      <td>46.611902</td>\n",
       "      <td>3.409529</td>\n",
       "      <td>6.415889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>000302_20170428_014929_crop</td>\n",
       "      <td>000302_20170428_014929_crop</td>\n",
       "      <td>-11.639558</td>\n",
       "      <td>110.811372</td>\n",
       "      <td>50.697859</td>\n",
       "      <td>3.394994</td>\n",
       "      <td>6.615739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>000303_20170428_014929_crop</td>\n",
       "      <td>000303_20170428_014929_crop</td>\n",
       "      <td>-11.638959</td>\n",
       "      <td>110.621205</td>\n",
       "      <td>80.981065</td>\n",
       "      <td>3.417410</td>\n",
       "      <td>6.409266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>000304_20170428_014929_crop</td>\n",
       "      <td>000304_20170428_014929_crop</td>\n",
       "      <td>-11.622172</td>\n",
       "      <td>110.624362</td>\n",
       "      <td>71.372729</td>\n",
       "      <td>3.442298</td>\n",
       "      <td>6.475294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000305_20170428_014929_crop</td>\n",
       "      <td>000305_20170428_014929_crop</td>\n",
       "      <td>-11.653182</td>\n",
       "      <td>110.842816</td>\n",
       "      <td>41.148574</td>\n",
       "      <td>3.362743</td>\n",
       "      <td>6.581986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000306_20170428_014929_crop</td>\n",
       "      <td>000306_20170428_014929_crop</td>\n",
       "      <td>-11.639653</td>\n",
       "      <td>110.848918</td>\n",
       "      <td>45.448889</td>\n",
       "      <td>3.418046</td>\n",
       "      <td>7.066340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000307_20170428_014929_crop</td>\n",
       "      <td>000307_20170428_014929_crop</td>\n",
       "      <td>-11.594655</td>\n",
       "      <td>110.667029</td>\n",
       "      <td>71.310654</td>\n",
       "      <td>3.419327</td>\n",
       "      <td>6.392989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000308_20170428_014929_crop</td>\n",
       "      <td>000308_20170428_014929_crop</td>\n",
       "      <td>-11.625097</td>\n",
       "      <td>110.781253</td>\n",
       "      <td>62.007056</td>\n",
       "      <td>3.445762</td>\n",
       "      <td>6.118000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000309_20170428_014929_crop</td>\n",
       "      <td>000309_20170428_014929_crop</td>\n",
       "      <td>-11.617632</td>\n",
       "      <td>110.819503</td>\n",
       "      <td>43.137900</td>\n",
       "      <td>3.374204</td>\n",
       "      <td>6.480609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>000310_20170428_014929_crop</td>\n",
       "      <td>000310_20170428_014929_crop</td>\n",
       "      <td>-11.615224</td>\n",
       "      <td>110.730418</td>\n",
       "      <td>57.135553</td>\n",
       "      <td>3.438944</td>\n",
       "      <td>6.480456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>000311_20170428_014929_crop</td>\n",
       "      <td>000311_20170428_014929_crop</td>\n",
       "      <td>-11.582266</td>\n",
       "      <td>110.659211</td>\n",
       "      <td>68.793361</td>\n",
       "      <td>3.416846</td>\n",
       "      <td>6.472687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000312_20170428_014929_crop</td>\n",
       "      <td>000312_20170428_014929_crop</td>\n",
       "      <td>-11.611417</td>\n",
       "      <td>110.632143</td>\n",
       "      <td>68.324372</td>\n",
       "      <td>3.449982</td>\n",
       "      <td>6.526206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>000313_20170428_014929_crop</td>\n",
       "      <td>000313_20170428_014929_crop</td>\n",
       "      <td>-11.581980</td>\n",
       "      <td>110.612070</td>\n",
       "      <td>82.608120</td>\n",
       "      <td>3.422010</td>\n",
       "      <td>6.597205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>000314_20170428_014929_crop</td>\n",
       "      <td>000314_20170428_014929_crop</td>\n",
       "      <td>-11.579352</td>\n",
       "      <td>110.872616</td>\n",
       "      <td>48.859596</td>\n",
       "      <td>3.398620</td>\n",
       "      <td>6.335582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>000315_20170428_014929_crop</td>\n",
       "      <td>000315_20170428_014929_crop</td>\n",
       "      <td>-11.715676</td>\n",
       "      <td>110.804665</td>\n",
       "      <td>51.000615</td>\n",
       "      <td>3.417725</td>\n",
       "      <td>6.178039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000316_20170428_014929_crop</td>\n",
       "      <td>000316_20170428_014929_crop</td>\n",
       "      <td>-11.627829</td>\n",
       "      <td>110.786739</td>\n",
       "      <td>54.600212</td>\n",
       "      <td>3.450524</td>\n",
       "      <td>6.244983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000317_20170428_014929_crop</td>\n",
       "      <td>000317_20170428_014929_crop</td>\n",
       "      <td>-11.623298</td>\n",
       "      <td>110.782047</td>\n",
       "      <td>48.158851</td>\n",
       "      <td>3.423865</td>\n",
       "      <td>6.889585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000318_20170428_014929_crop</td>\n",
       "      <td>000318_20170428_014929_crop</td>\n",
       "      <td>-11.584216</td>\n",
       "      <td>110.848558</td>\n",
       "      <td>48.809672</td>\n",
       "      <td>3.370561</td>\n",
       "      <td>6.760527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000319_20170428_014929_crop</td>\n",
       "      <td>000319_20170428_014929_crop</td>\n",
       "      <td>-11.599203</td>\n",
       "      <td>110.797821</td>\n",
       "      <td>56.177316</td>\n",
       "      <td>3.385294</td>\n",
       "      <td>6.173338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>000320_20170428_014929_crop</td>\n",
       "      <td>000320_20170428_014929_crop</td>\n",
       "      <td>-11.620945</td>\n",
       "      <td>110.950061</td>\n",
       "      <td>43.231876</td>\n",
       "      <td>3.405404</td>\n",
       "      <td>6.398106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>000321_20170428_014929_crop</td>\n",
       "      <td>000321_20170428_014929_crop</td>\n",
       "      <td>-11.536389</td>\n",
       "      <td>110.736439</td>\n",
       "      <td>53.561181</td>\n",
       "      <td>3.467152</td>\n",
       "      <td>6.546848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>000322_20170428_014929_crop</td>\n",
       "      <td>000322_20170428_014929_crop</td>\n",
       "      <td>-11.586010</td>\n",
       "      <td>110.850233</td>\n",
       "      <td>47.709875</td>\n",
       "      <td>3.384137</td>\n",
       "      <td>6.878157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>000323_20170428_014929_crop</td>\n",
       "      <td>000323_20170428_014929_crop</td>\n",
       "      <td>-11.591717</td>\n",
       "      <td>110.706691</td>\n",
       "      <td>59.894157</td>\n",
       "      <td>3.458786</td>\n",
       "      <td>6.607329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>000324_20170428_014929_crop</td>\n",
       "      <td>000324_20170428_014929_crop</td>\n",
       "      <td>-11.635761</td>\n",
       "      <td>110.813986</td>\n",
       "      <td>52.219211</td>\n",
       "      <td>3.412037</td>\n",
       "      <td>6.105341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>000325_20170428_014929_crop</td>\n",
       "      <td>000325_20170428_014929_crop</td>\n",
       "      <td>-11.585297</td>\n",
       "      <td>110.737009</td>\n",
       "      <td>46.802642</td>\n",
       "      <td>3.463149</td>\n",
       "      <td>7.024770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>000326_20170428_014929_crop</td>\n",
       "      <td>000326_20170428_014929_crop</td>\n",
       "      <td>-11.581101</td>\n",
       "      <td>110.787343</td>\n",
       "      <td>50.801173</td>\n",
       "      <td>3.423935</td>\n",
       "      <td>6.630812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>000327_20170428_014929_crop</td>\n",
       "      <td>000327_20170428_014929_crop</td>\n",
       "      <td>-11.567802</td>\n",
       "      <td>110.679745</td>\n",
       "      <td>52.085413</td>\n",
       "      <td>3.434920</td>\n",
       "      <td>7.211933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>000328_20170428_014929_crop</td>\n",
       "      <td>000328_20170428_014929_crop</td>\n",
       "      <td>-11.616599</td>\n",
       "      <td>110.516372</td>\n",
       "      <td>85.937146</td>\n",
       "      <td>3.449996</td>\n",
       "      <td>6.915638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>000329_20170428_014929_crop</td>\n",
       "      <td>000329_20170428_014929_crop</td>\n",
       "      <td>-11.587660</td>\n",
       "      <td>110.533880</td>\n",
       "      <td>84.013837</td>\n",
       "      <td>3.479848</td>\n",
       "      <td>6.441905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     flag_start                     flag_end  pred_multi_lat  \\\n",
       "0   000056_20170428_014929_crop  000056_20170428_014929_crop      -11.775477   \n",
       "1   000060_20170428_014929_crop  000060_20170428_014929_crop      -11.737770   \n",
       "2   000061_20170428_014929_crop  000061_20170428_014929_crop      -11.739782   \n",
       "3   000062_20170428_014929_crop  000062_20170428_014929_crop      -11.732347   \n",
       "4   000065_20170428_014929_crop  000065_20170428_014929_crop      -11.697810   \n",
       "5   000066_20170428_014929_crop  000066_20170428_014929_crop      -11.684561   \n",
       "6   000081_20170428_014929_crop  000081_20170428_014929_crop      -11.794250   \n",
       "7   000296_20170428_014929_crop  000296_20170428_014929_crop      -11.601108   \n",
       "8   000298_20170428_014929_crop  000298_20170428_014929_crop      -11.527499   \n",
       "9   000299_20170428_014929_crop  000299_20170428_014929_crop      -11.674957   \n",
       "10  000300_20170428_014929_crop  000300_20170428_014929_crop      -11.667279   \n",
       "11  000301_20170428_014929_crop  000301_20170428_014929_crop      -11.598023   \n",
       "12  000302_20170428_014929_crop  000302_20170428_014929_crop      -11.639558   \n",
       "13  000303_20170428_014929_crop  000303_20170428_014929_crop      -11.638959   \n",
       "14  000304_20170428_014929_crop  000304_20170428_014929_crop      -11.622172   \n",
       "15  000305_20170428_014929_crop  000305_20170428_014929_crop      -11.653182   \n",
       "16  000306_20170428_014929_crop  000306_20170428_014929_crop      -11.639653   \n",
       "17  000307_20170428_014929_crop  000307_20170428_014929_crop      -11.594655   \n",
       "18  000308_20170428_014929_crop  000308_20170428_014929_crop      -11.625097   \n",
       "19  000309_20170428_014929_crop  000309_20170428_014929_crop      -11.617632   \n",
       "20  000310_20170428_014929_crop  000310_20170428_014929_crop      -11.615224   \n",
       "21  000311_20170428_014929_crop  000311_20170428_014929_crop      -11.582266   \n",
       "22  000312_20170428_014929_crop  000312_20170428_014929_crop      -11.611417   \n",
       "23  000313_20170428_014929_crop  000313_20170428_014929_crop      -11.581980   \n",
       "24  000314_20170428_014929_crop  000314_20170428_014929_crop      -11.579352   \n",
       "25  000315_20170428_014929_crop  000315_20170428_014929_crop      -11.715676   \n",
       "26  000316_20170428_014929_crop  000316_20170428_014929_crop      -11.627829   \n",
       "27  000317_20170428_014929_crop  000317_20170428_014929_crop      -11.623298   \n",
       "28  000318_20170428_014929_crop  000318_20170428_014929_crop      -11.584216   \n",
       "29  000319_20170428_014929_crop  000319_20170428_014929_crop      -11.599203   \n",
       "30  000320_20170428_014929_crop  000320_20170428_014929_crop      -11.620945   \n",
       "31  000321_20170428_014929_crop  000321_20170428_014929_crop      -11.536389   \n",
       "32  000322_20170428_014929_crop  000322_20170428_014929_crop      -11.586010   \n",
       "33  000323_20170428_014929_crop  000323_20170428_014929_crop      -11.591717   \n",
       "34  000324_20170428_014929_crop  000324_20170428_014929_crop      -11.635761   \n",
       "35  000325_20170428_014929_crop  000325_20170428_014929_crop      -11.585297   \n",
       "36  000326_20170428_014929_crop  000326_20170428_014929_crop      -11.581101   \n",
       "37  000327_20170428_014929_crop  000327_20170428_014929_crop      -11.567802   \n",
       "38  000328_20170428_014929_crop  000328_20170428_014929_crop      -11.616599   \n",
       "39  000329_20170428_014929_crop  000329_20170428_014929_crop      -11.587660   \n",
       "40                          NaN                          NaN             NaN   \n",
       "\n",
       "    pred_multi_long  pred_multi_depth  pred_multi_magnitude  pred_multi_time  \n",
       "0        110.481090         98.450831              3.378351         6.219618  \n",
       "1        110.509147         94.968034              3.367091         5.999084  \n",
       "2        110.538836         90.676946              3.357709         6.269854  \n",
       "3        110.506474        102.330615              3.370601         6.599156  \n",
       "4        110.505394         94.617716              3.356358         6.334960  \n",
       "5        110.464881        101.259360              3.360927         6.706778  \n",
       "6        110.444291         95.127546              3.410674         5.795427  \n",
       "7        110.645147         73.316677              3.422051         6.635134  \n",
       "8        110.594832         75.137481              3.408921         6.425026  \n",
       "9        110.768655         45.477430              3.432624         6.146806  \n",
       "10       110.795269         41.931993              3.410157         6.711325  \n",
       "11       110.785566         46.611902              3.409529         6.415889  \n",
       "12       110.811372         50.697859              3.394994         6.615739  \n",
       "13       110.621205         80.981065              3.417410         6.409266  \n",
       "14       110.624362         71.372729              3.442298         6.475294  \n",
       "15       110.842816         41.148574              3.362743         6.581986  \n",
       "16       110.848918         45.448889              3.418046         7.066340  \n",
       "17       110.667029         71.310654              3.419327         6.392989  \n",
       "18       110.781253         62.007056              3.445762         6.118000  \n",
       "19       110.819503         43.137900              3.374204         6.480609  \n",
       "20       110.730418         57.135553              3.438944         6.480456  \n",
       "21       110.659211         68.793361              3.416846         6.472687  \n",
       "22       110.632143         68.324372              3.449982         6.526206  \n",
       "23       110.612070         82.608120              3.422010         6.597205  \n",
       "24       110.872616         48.859596              3.398620         6.335582  \n",
       "25       110.804665         51.000615              3.417725         6.178039  \n",
       "26       110.786739         54.600212              3.450524         6.244983  \n",
       "27       110.782047         48.158851              3.423865         6.889585  \n",
       "28       110.848558         48.809672              3.370561         6.760527  \n",
       "29       110.797821         56.177316              3.385294         6.173338  \n",
       "30       110.950061         43.231876              3.405404         6.398106  \n",
       "31       110.736439         53.561181              3.467152         6.546848  \n",
       "32       110.850233         47.709875              3.384137         6.878157  \n",
       "33       110.706691         59.894157              3.458786         6.607329  \n",
       "34       110.813986         52.219211              3.412037         6.105341  \n",
       "35       110.737009         46.802642              3.463149         7.024770  \n",
       "36       110.787343         50.801173              3.423935         6.630812  \n",
       "37       110.679745         52.085413              3.434920         7.211933  \n",
       "38       110.516372         85.937146              3.449996         6.915638  \n",
       "39       110.533880         84.013837              3.479848         6.441905  \n",
       "40              NaN               NaN                   NaN              NaN  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_csv(model_save_path+folder_save_name+\"/\"+'Result_test-{}check.csv'.format(ncheck),index=False) \n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event</th>\n",
       "      <th>pred_lat</th>\n",
       "      <th>target_lat</th>\n",
       "      <th>pred_long</th>\n",
       "      <th>target_long</th>\n",
       "      <th>pred_Depth</th>\n",
       "      <th>target_Depth</th>\n",
       "      <th>pred_Mag</th>\n",
       "      <th>target_Mag</th>\n",
       "      <th>pred_Time</th>\n",
       "      <th>target_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170428_014929crop</td>\n",
       "      <td>-11.632288</td>\n",
       "      <td>-8.08697</td>\n",
       "      <td>110.697352</td>\n",
       "      <td>111.716</td>\n",
       "      <td>64.668386</td>\n",
       "      <td>126.932</td>\n",
       "      <td>3.412838</td>\n",
       "      <td>3.4</td>\n",
       "      <td>6.495468</td>\n",
       "      <td>16.024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 event   pred_lat  target_lat   pred_long  target_long  \\\n",
       "0  20170428_014929crop -11.632288    -8.08697  110.697352      111.716   \n",
       "\n",
       "   pred_Depth  target_Depth  pred_Mag  target_Mag  pred_Time  target_Time  \n",
       "0   64.668386       126.932  3.412838         3.4   6.495468       16.024  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RESULT FOR MULTI-OUTPUT MODEL (LATITUDE, LONGITUDE, DEPTH, MAGNITUDE, ORIGIN TIME)\n",
    "df_test['foldername'] = df_test.flag_start.apply(lambda x: str(x)[7:-5]+'crop')\n",
    "df_result = df_test.dropna()\n",
    "df_result = df_result.groupby('foldername').agg('mean').reset_index()\n",
    "df_result = df_result.merge(df[['foldername','Lat','Long','Depth(km)','Mag','time']], on='foldername')\n",
    "df_result = df_result[['foldername','pred_multi_lat','Lat','pred_multi_long','Long','pred_multi_depth','Depth(km)','pred_multi_magnitude','Mag','pred_multi_time','time']]\n",
    "df_result.columns = ['event', 'pred_lat', 'target_lat', 'pred_long', 'target_long', 'pred_Depth', 'target_Depth', 'pred_Mag', 'target_Mag', 'pred_Time', 'target_Time']\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mobilenet_classification_192.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
